{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"HW07-BERT.ipynb","provenance":[],"collapsed_sections":["Fm0rpTHq0e4N","Ws8c8_4d5UCI","5_H1kqhR8CdM","rzHQit6eMnKG","SbIyDlMjS1Ej","nbC0VFgLS1El","7tnGF3VhS1En","kMmdLOKBMsdE"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# **Homework 7 - Bert (Question Answering)**\n","\n","If you have any questions, feel free to email us at mlta-2022-spring@googlegroups.com\n","\n","\n","\n","Slide:    [Link](https://docs.google.com/presentation/d/1H5ZONrb2LMOCixLY7D5_5-7LkIaXO6AGEaV2mRdTOMY/edit?usp=sharing)　Kaggle: [Link](https://www.kaggle.com/c/ml2022spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb)　Ref: [Link](https://github.com/pai4451/ML2021/tree/main/hw7)\n","\n","\n"],"metadata":{"id":"xvSGDbExff_I"}},{"cell_type":"markdown","source":["## Task description\n","- Chinese Extractive Question Answering\n","  - Input: Paragraph + Question\n","  - Output: Answer\n","\n","- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n","\n","- Todo\n","    - Fine tune a pretrained chinese BERT model\n","    - Change hyperparameters (e.g. doc_stride)\n","    - Apply linear learning rate decay\n","    - Try other pretrained models\n","    - Improve preprocessing\n","    - Improve postprocessing\n","- Training tips\n","    - Automatic mixed precision\n","    - Gradient accumulation\n","    - Ensemble\n","\n","- Estimated training time (tesla t4 with automatic mixed precision enabled)\n","    - Simple: 8mins\n","    - Medium: 8mins\n","    - Strong: 25mins\n","    - Boss: 2.5hrs\n","  "],"metadata":{"id":"WGOr_eS3wJJf"}},{"cell_type":"code","source":["# For this HW, K80 < P4 < T4 < P100 <= T4(fp16) < V100\n","!nvidia-smi"],"metadata":{"id":"kRelB41XS1Dq","execution":{"iopub.status.busy":"2022-04-30T02:54:21.701688Z","iopub.execute_input":"2022-04-30T02:54:21.70222Z","iopub.status.idle":"2022-04-30T02:54:22.403797Z","shell.execute_reply.started":"2022-04-30T02:54:21.702184Z","shell.execute_reply":"2022-04-30T02:54:22.402991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Download Dataset"],"metadata":{"id":"TJ1fSAJE2oaC"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","import os\n","\n","# Download link 1\n","!gdown --id '1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb' --output hw7_data.zip\n","\n","# Download Link 2 (if the above link fails) \n","# !gdown --id '1qwjbRjq481lHsnTrrF4OjKQnxzgoLEFR' --output hw7_data.zip\n","\n","# Download Link 3 (if the above link fails) \n","# !gdown --id '1QXuWjNRZH6DscSd6QcRER0cnxmpZvijn' --output hw7_data.zip\n","\n","!unzip -o hw7_data.zip\n","\n","dev_path = os.path.join(\".\", \"hw7_dev.json\")\n","test_path = os.path.join(\".\", \"hw7_test.json\")\n","train_path = os.path.join(\".\", \"hw7_train.json\")"],"metadata":{"id":"YPrc4Eie9Yo5","execution":{"iopub.status.busy":"2022-04-30T02:54:23.310834Z","iopub.execute_input":"2022-04-30T02:54:23.311395Z","iopub.status.idle":"2022-04-30T02:54:23.331313Z","shell.execute_reply.started":"2022-04-30T02:54:23.311355Z","shell.execute_reply":"2022-04-30T02:54:23.330436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# kaggle\n","kaggle_data_folder = \"../input/ml2022spring-hw7\"\n","\n","dev_path = os.path.join(kaggle_data_folder, \"hw7_dev.json\")\n","test_path = os.path.join(kaggle_data_folder, \"hw7_test.json\")\n","train_path = os.path.join(kaggle_data_folder, \"hw7_train.json\")"],"metadata":{"id":"LsdVfOr-S1Dw","execution":{"iopub.status.busy":"2022-04-30T02:54:25.085304Z","iopub.execute_input":"2022-04-30T02:54:25.085751Z","iopub.status.idle":"2022-04-30T02:54:25.09282Z","shell.execute_reply.started":"2022-04-30T02:54:25.085717Z","shell.execute_reply":"2022-04-30T02:54:25.092137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Install transformers\n","\n","Documentation for the toolkit:　https://huggingface.co/transformers/"],"metadata":{"id":"TevOvhC03m0h"}},{"cell_type":"code","source":["# You are allowed to change version of transformers or use other toolkits\n","!pip install transformers==4.5.0"],"metadata":{"id":"tbxWFX_jpDom","execution":{"iopub.status.busy":"2022-04-30T02:54:27.067597Z","iopub.execute_input":"2022-04-30T02:54:27.067886Z","iopub.status.idle":"2022-04-30T02:54:41.400874Z","shell.execute_reply.started":"2022-04-30T02:54:27.067855Z","shell.execute_reply":"2022-04-30T02:54:41.400001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Packages"],"metadata":{"id":"8dKM4yCh4LI_"}},{"cell_type":"code","source":["import json\n","import numpy as np\n","import random\n","import torch\n","from torch.utils.data import DataLoader, Dataset \n","from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast\n","\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Fix random seed for reproducibility\n","def same_seeds(seed):\n","      torch.manual_seed(seed)\n","      if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed)\n","            torch.cuda.manual_seed_all(seed)\n","      np.random.seed(seed)\n","      random.seed(seed)\n","      torch.backends.cudnn.benchmark = False\n","      torch.backends.cudnn.deterministic = True\n","same_seeds(0)\n","\n","device"],"metadata":{"id":"WOTHHtWJoahe","execution":{"iopub.status.busy":"2022-04-30T02:54:41.405874Z","iopub.execute_input":"2022-04-30T02:54:41.406574Z","iopub.status.idle":"2022-04-30T02:54:46.946468Z","shell.execute_reply.started":"2022-04-30T02:54:41.406528Z","shell.execute_reply":"2022-04-30T02:54:46.945667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)    \n","fp16_training = True\n","\n","if fp16_training:\n","    !pip install accelerate==0.2.0\n","    from accelerate import Accelerator\n","    accelerator = Accelerator(fp16=True)\n","    device = accelerator.device\n","\n","# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"],"metadata":{"id":"7pBtSZP1SKQO","execution":{"iopub.status.busy":"2022-04-30T02:54:46.947916Z","iopub.execute_input":"2022-04-30T02:54:46.948185Z","iopub.status.idle":"2022-04-30T02:54:56.110827Z","shell.execute_reply.started":"2022-04-30T02:54:46.948148Z","shell.execute_reply":"2022-04-30T02:54:56.109875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Model and Tokenizer\n","\n","\n","\n","\n"," "],"metadata":{"id":"2YgXHuVLp_6j"}},{"cell_type":"code","source":["model_name = \"hfl/chinese-macbert-large\"\n","# model_name = \"Langboat/mengzi-bert-base\"\n","model = BertForQuestionAnswering.from_pretrained(model_name).to(device)\n","tokenizer = BertTokenizerFast.from_pretrained(model_name)\n","\n","# model = BertForQuestionAnswering.from_pretrained(\"bert-base-chinese\").to(device)\n","# tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n","\n","# model = BertForQuestionAnswering.from_pretrained(\"hfl/chinese-roberta-wwm-ext\").to(device)\n","# tokenizer = BertTokenizerFast.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n","\n","# model = BertForQuestionAnswering.from_pretrained(\"hfl/chinese-bert-wwm-ext\").to(device)\n","# tokenizer = BertTokenizerFast.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n","\n","# model = BertForQuestionAnswering.from_pretrained(\"Langboat/mengzi-bert-base\").to(device)\n","# tokenizer = BertTokenizerFast.from_pretrained(\"Langboat/mengzi-bert-base\")\n","\n","# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"],"metadata":{"id":"xyBCYGjAp3ym","execution":{"iopub.status.busy":"2022-04-30T02:54:56.116289Z","iopub.execute_input":"2022-04-30T02:54:56.118379Z","iopub.status.idle":"2022-04-30T02:55:45.07913Z","shell.execute_reply.started":"2022-04-30T02:54:56.11833Z","shell.execute_reply":"2022-04-30T02:55:45.078378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Read Data\n","\n","- Training set: 31690 QA pairs\n","- Dev set: 4131  QA pairs\n","- Test set: 4957  QA pairs\n","\n","- {train/dev/test}_questions:    \n","  - List of dicts with the following keys:\n","   - id (int)\n","   - paragraph_id (int)\n","   - question_text (string)\n","   - answer_text (string)\n","   - answer_start (int)\n","   - answer_end (int)\n","- {train/dev/test}_paragraphs: \n","  - List of strings\n","  - paragraph_ids in questions correspond to indexs in paragraphs\n","  - A paragraph may be used by several questions "],"metadata":{"id":"3Td-GTmk5OW4"}},{"cell_type":"code","source":["!pip install torchinfo\n","from torchinfo import summary\n","\n","summary(model)"],"metadata":{"id":"JPxbGSeRS1D-","execution":{"iopub.status.busy":"2022-04-25T08:08:42.43975Z","iopub.execute_input":"2022-04-25T08:08:42.440004Z","iopub.status.idle":"2022-04-25T08:08:51.988975Z","shell.execute_reply.started":"2022-04-25T08:08:42.439969Z","shell.execute_reply":"2022-04-25T08:08:51.988194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_data(file):\n","    with open(file, 'r', encoding=\"utf-8\") as reader:\n","        data = json.load(reader)\n","    return data[\"questions\"], data[\"paragraphs\"]\n","\n","train_questions, train_paragraphs = read_data(train_path)\n","dev_questions, dev_paragraphs = read_data(dev_path)\n","test_questions, test_paragraphs = read_data(test_path)"],"metadata":{"id":"NvX7hlepogvu","execution":{"iopub.status.busy":"2022-04-30T02:55:45.080637Z","iopub.execute_input":"2022-04-30T02:55:45.081133Z","iopub.status.idle":"2022-04-30T02:55:45.831752Z","shell.execute_reply.started":"2022-04-30T02:55:45.081093Z","shell.execute_reply":"2022-04-30T02:55:45.830993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tokenize Data"],"metadata":{"id":"Fm0rpTHq0e4N"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","# test_paragraphs = [\"2022白堊紀滅絕事件\", \"杭州筧橋機場\", \"蔡鍔\", \"丁旿\"]\n","from itertools import chain\n","\n","unks = set();\n","for p in chain(train_paragraphs, dev_paragraphs, test_paragraphs):\n","    t = tokenizer(p, return_offsets_mapping=True)\n","#     print(p)\n","#     print(t['input_ids'])\n","#     print(t['offset_mapping'])\n","    unks |= {p[pos[0]:pos[1]] for token_id, pos in zip(t['input_ids'], t['offset_mapping']) if token_id == 100}\n","\n","print(unks)\n","\n","for unk in unks:\n","    for c in unk:\n","        tokenizer.add_tokens(unk)\n","    \n","model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"GQz29-iMS1EK","execution":{"iopub.status.busy":"2022-04-30T02:55:45.833005Z","iopub.execute_input":"2022-04-30T02:55:45.833262Z","iopub.status.idle":"2022-04-30T02:55:45.903607Z","shell.execute_reply.started":"2022-04-30T02:55:45.833227Z","shell.execute_reply":"2022-04-30T02:55:45.902734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","unks_check = set();\n","for p in chain(train_paragraphs, dev_paragraphs, test_paragraphs):\n","    t = tokenizer(p, return_offsets_mapping=True)\n","    unks_check |= {p[pos[0]:pos[1]] for token_id, pos in zip(t['input_ids'], t['offset_mapping']) if token_id == 100}\n","\n","print(unks_check)\n","print(unks & unks_check)"],"metadata":{"id":"u_ytV4wnS1EN","execution":{"iopub.status.busy":"2022-04-25T13:18:35.554621Z","iopub.execute_input":"2022-04-25T13:18:35.556217Z","iopub.status.idle":"2022-04-25T13:18:35.624173Z","shell.execute_reply.started":"2022-04-25T13:18:35.556168Z","shell.execute_reply":"2022-04-25T13:18:35.623154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize questions and paragraphs separately\n","# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n","\n","train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n","dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n","test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n","\n","train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False, return_offsets_mapping=True)\n","dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False, return_offsets_mapping=True)\n","test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False, return_offsets_mapping=True)\n","\n","# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"],"metadata":{"id":"rTZ6B70Hoxie","execution":{"iopub.status.busy":"2022-04-30T02:55:45.906375Z","iopub.execute_input":"2022-04-30T02:55:45.90664Z","iopub.status.idle":"2022-04-30T02:56:02.802836Z","shell.execute_reply.started":"2022-04-30T02:55:45.906605Z","shell.execute_reply":"2022-04-30T02:56:02.802097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset and Dataloader"],"metadata":{"id":"Ws8c8_4d5UCI"}},{"cell_type":"code","source":["import random\n","\n","class QA_Dataset(Dataset):\n","    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n","        self.split = split\n","        self.questions = questions\n","        self.tokenized_questions = tokenized_questions\n","        self.tokenized_paragraphs = tokenized_paragraphs\n","        self.max_question_len = 81\n","        self.max_paragraph_len = 320\n","        \n","        ##### TODO: Change value of doc_stride #####\n","        self.doc_stride = 270\n","\n","        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n","        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        question = self.questions[idx]\n","        tokenized_question = self.tokenized_questions[idx]\n","        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n","\n","        ##### TODO: Preprocessing #####\n","        # Hint: How to prevent model from learning something it should not learn\n","        # like answer is always at the center\n","\n","        if self.split == \"train\":\n","            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n","            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n","            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n","\n","            # random at least 1/4 paragraph\n","            paragraph_start = random.randint(\n","                answer_start_token + self.max_paragraph_len // 4 - self.max_paragraph_len,\n","                answer_end_token - self.max_paragraph_len // 4,\n","            )\n","            # take more paragraph as many as possible\n","            paragraph_start = max(0, min(paragraph_start, len(tokenized_paragraph) - self.max_paragraph_len))\n","            paragraph_end = paragraph_start + self.max_paragraph_len\n","            \n","            check = paragraph_start <= answer_start_token < paragraph_end and paragraph_start <= answer_end_token < paragraph_end\n","            if not check:\n","                print(\"question:\", tokenizer.decode(tokenized_question.ids[:self.max_question_len]).replace(\" \", \"\"))\n","                print(\"answer:\", tokenizer.decode(tokenized_paragraph.ids[answer_start_token : answer_end_token+1]).replace(\" \", \"\"))\n","                print(\"paraph:\", tokenizer.decode(tokenized_paragraph.ids[paragraph_start : paragraph_end]).replace(\" \", \"\"))\n","                print(\"\")\n","            assert(check)\n","            \n","            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n","            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n","            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]     \n","            \n","            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n","            answer_start_token += len(input_ids_question) - paragraph_start\n","            answer_end_token += len(input_ids_question) - paragraph_start\n","            \n","            # Pad sequence and obtain inputs to model \n","            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n","            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n","\n","        # Validation/Testing\n","        else:\n","            input_ids_list, token_type_ids_list, attention_mask_list, offsets_list = [], [], [], []\n","            \n","            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n","            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n","                \n","                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n","                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n","                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n","                \n","                # Pad sequence and obtain inputs to model\n","                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n","                \n","                input_ids_list.append(input_ids)\n","                token_type_ids_list.append(token_type_ids)\n","                attention_mask_list.append(attention_mask)\n","\n","            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n","\n","    def padding(self, input_ids_question, input_ids_paragraph):\n","        # Pad zeros if sequence length is shorter than max_seq_len\n","        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n","        # Indices of input sequence tokens in the vocabulary\n","        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n","        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n","        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n","        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n","        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n","        \n","        return input_ids, token_type_ids, attention_mask\n","\n","train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n","dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n","test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n","\n","train_batch_size = 8\n","\n","# Note: Do NOT change batch size of dev_loader / test_loader !\n","# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n","train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n","dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n","test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"],"metadata":{"id":"Xjooag-Swnuh","execution":{"iopub.status.busy":"2022-04-30T02:56:02.80427Z","iopub.execute_input":"2022-04-30T02:56:02.804517Z","iopub.status.idle":"2022-04-30T02:56:02.830004Z","shell.execute_reply.started":"2022-04-30T02:56:02.804483Z","shell.execute_reply":"2022-04-30T02:56:02.829287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check train data\n","for i, data in enumerate(train_loader):\n","    pass\n","\n","# check question lenth\n","for i, data in enumerate(train_loader):\n","    if i in [1152]:\n","        # wrong data\n","        continue\n","    if len(train_questions_tokenized[i].ids) > train_set.max_question_len:\n","        print(i, train_questions[i]['question_text'])\n","        print(train_questions_tokenized[i].tokens)\n","        print(f\"{len(train_questions_tokenized[i].ids)} > max_question_len:{train_set.max_question_len}\")\n","        raise\n","        \n","for i, data in enumerate(dev_loader):\n","    if len(dev_questions_tokenized[i].ids) > dev_set.max_question_len:\n","        print(i, dev_questions[i]['question_text'])\n","        print(dev_questions_tokenized[i].tokens)\n","        print(f\"{len(dev_questions_tokenized[i].ids)} > max_question_len:{dev_set.max_question_len}\")\n","        raise\n","        \n","for i, data in enumerate(test_loader):\n","    if len(test_questions_tokenized[i].ids) > test_set.max_question_len:\n","        print(i, test_questions[i]['question_text'])\n","        print(test_questions_tokenized[i].tokens)\n","        print(f\"{len(test_questions_tokenized[i].ids)} > max_question_len:{test_set.max_question_len}\")\n","        raise"],"metadata":{"id":"Tx5AnUpHsoeo","execution":{"iopub.status.busy":"2022-04-25T08:09:09.229774Z","iopub.execute_input":"2022-04-25T08:09:09.230269Z","iopub.status.idle":"2022-04-25T08:09:27.248193Z","shell.execute_reply.started":"2022-04-25T08:09:09.230219Z","shell.execute_reply":"2022-04-25T08:09:27.24748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Function for Evaluation"],"metadata":{"id":"5_H1kqhR8CdM"}},{"cell_type":"code","source":["def get_answer(p, offsets, start, end):\n","    if start is None or end is None:\n","        return \"\"\n","    \n","    return p[offsets[start][0]:offsets[end][1]]"],"metadata":{"id":"-CO_oGbuS1ER","execution":{"iopub.status.busy":"2022-04-30T02:56:02.831193Z","iopub.execute_input":"2022-04-30T02:56:02.831867Z","iopub.status.idle":"2022-04-30T02:56:02.837118Z","shell.execute_reply.started":"2022-04-30T02:56:02.83183Z","shell.execute_reply":"2022-04-30T02:56:02.836219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(data, output, doc_stride, paragraph, paragraph_tokenized, offsets):\n","    ##### TODO: Postprocessing #####\n","    # There is a bug and room for improvement in postprocessing \n","    # Hint: Open your prediction file to see what is wrong \n","    \n","    answer = ''\n","    max_prob = float('-inf')\n","    max_prob_bak = float('-inf')\n","    num_of_windows = data[0].shape[1]\n","    \n","    # index in the whole tokens (not just relative to window)\n","    entire_start_index = None\n","    entire_end_index = None\n","    \n","    for k in range(num_of_windows):            \n","        # print('window',k)\n","        # Obtain answer by choosing the most probable start position / end position\n","        mask = data[1][0][k].bool() & data[2][0][k].bool() # token type & attention mask\n","        masked_output_start = torch.masked_select(output.start_logits[k].cpu(), mask)[:-1] # -1 is [SEP]\n","        masked_output_end = torch.masked_select(output.end_logits[k].cpu(), mask)[:-1] # -1 is [SEP]\n","        \n","        start_prob, start_index = torch.max(masked_output_start, dim=0)     \n","        end_prob, end_index = torch.max(masked_output_end[start_index:], dim=0)\n","        end_index += start_index\n","        # model output means include end_index\n","        # so answer is p[start_index, end_index]\n","        \n","        # Probability of answer is calculated as sum of start_prob and end_prob\n","        prob = start_prob + end_prob\n","#         masked_data = torch.masked_select(data[0][0][k], mask)[:-1] # -1 is [SEP]\n","\n","        # Replace answer if calculated probability is larger than previous windows\n","        if (prob > max_prob) and (end_index - start_index <= 30):\n","            max_prob = prob\n","            entire_start_index = start_index.item() + doc_stride * k\n","            entire_end_index = end_index.item() + doc_stride * k\n","            # print('entire_start_index',entire_start_index)\n","            # print('entire_end_index',entire_end_index)\n","#             # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n","#             answer = tokenizer.decode(masked_data[start_index : end_index + 1])\n","\n","    answer = get_answer(paragraph, offsets, entire_start_index, entire_end_index)\n","    return answer.strip()"],"metadata":{"id":"SqeA3PLPxOHu","execution":{"iopub.status.busy":"2022-04-25T13:18:46.868869Z","iopub.execute_input":"2022-04-25T13:18:46.869224Z","iopub.status.idle":"2022-04-25T13:18:46.882828Z","shell.execute_reply.started":"2022-04-25T13:18:46.869178Z","shell.execute_reply":"2022-04-25T13:18:46.881836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"rzHQit6eMnKG"}},{"cell_type":"markdown","source":["## Optimizer: Adam + lr scheduling\n","Inverse square root scheduling is important to the stability when training Transformer. It's later used on RNN as well.\n","Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.\n","$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$\n","$$lrate = \\frac{1}{\\sqrt{d_{\\text{model}}}}\\cdot\\min(\\frac{1}{\\sqrt{{step\\_num}}}, \\frac{{step\\_num}}{{warmup\\_steps}}\\cdot\\frac{1}{\\sqrt{{warmup\\_steps}}})$$"],"metadata":{"id":"SbIyDlMjS1Ej"}},{"cell_type":"code","source":["def get_rate(d_model, step_num, warmup_step):\n","    # TODO: Change lr from constant to the equation shown above\n","    lr = d_model**(-0.5)*min(step_num**(-0.5), step_num*warmup_step**(-1.5))\n","    return lr"],"metadata":{"id":"yXAcDFnKS1Ej","execution":{"iopub.status.busy":"2022-04-25T08:09:27.269406Z","iopub.execute_input":"2022-04-25T08:09:27.269712Z","iopub.status.idle":"2022-04-25T08:09:27.280067Z","shell.execute_reply.started":"2022-04-25T08:09:27.269677Z","shell.execute_reply":"2022-04-25T08:09:27.27942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NoamOpt:\n","    \"Optim wrapper that implements rate.\"\n","    def __init__(self, model_size, factor, warmup, optimizer):\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self.warmup = warmup\n","        self.factor = factor\n","        self.model_size = model_size\n","        self._rate = 0\n","    \n","    @property\n","    def param_groups(self):\n","        return self.optimizer.param_groups\n","        \n","    def step(self):\n","        \"Update parameters and rate\"\n","        self._step += 1\n","        rate = self.rate()\n","        for p in self.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","        self.optimizer.step()\n","        \n","    def rate(self, step = None):\n","        \"Implement `lrate` above\"\n","        if step is None:\n","            step = self._step\n","        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)\n","\n","    def zero_grad(self):\n","        self.optimizer.zero_grad()"],"metadata":{"id":"e0IktVJ7S1El","execution":{"iopub.status.busy":"2022-04-25T08:09:27.281633Z","iopub.execute_input":"2022-04-25T08:09:27.282213Z","iopub.status.idle":"2022-04-25T08:09:27.292064Z","shell.execute_reply.started":"2022-04-25T08:09:27.282174Z","shell.execute_reply":"2022-04-25T08:09:27.291297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hyperparameter"],"metadata":{"id":"nbC0VFgLS1El"}},{"cell_type":"code","source":["num_epoch = 3\n","validation = False\n","logging_step = 100\n","learning_rate = 1e-5\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","# warm up\n","lr_factor = 2.0\n","lr_warmup = 100"],"metadata":{"id":"e30usJZ7S1Em","execution":{"iopub.status.busy":"2022-04-25T08:09:27.293553Z","iopub.execute_input":"2022-04-25T08:09:27.294073Z","iopub.status.idle":"2022-04-25T08:09:27.306205Z","shell.execute_reply.started":"2022-04-25T08:09:27.294035Z","shell.execute_reply":"2022-04-25T08:09:27.305559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Scheduling Visualized"],"metadata":{"id":"7tnGF3VhS1En"}},{"cell_type":"code","source":["optimizer_decay = NoamOpt(\n","    model_size=1200000, \n","    factor=lr_factor, \n","    warmup=lr_warmup, \n","    optimizer=AdamW(model.parameters(), lr=0)\n",")\n","\n","total = 1000\n","plt.plot(np.arange(1, total), [optimizer_decay.rate(i) for i in range(1, total)])\n","plt.legend([f\"{optimizer_decay.model_size}:{optimizer_decay.warmup}\"])\n","\n","# optimizer = optimizer_decay"],"metadata":{"id":"m3u46Ln0S1Eo","execution":{"iopub.status.busy":"2022-04-25T08:09:27.30715Z","iopub.execute_input":"2022-04-25T08:09:27.309522Z","iopub.status.idle":"2022-04-25T08:09:27.54067Z","shell.execute_reply.started":"2022-04-25T08:09:27.309417Z","shell.execute_reply":"2022-04-25T08:09:27.539989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import get_polynomial_decay_schedule_with_warmup\n","\n","def show_plot():\n","    optimizer = torch.optim.SGD(torch.nn.Linear(2, 1).parameters(), lr=learning_rate)\n","    total = num_epoch * len(train_loader)\n","    scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, total//10, total, lr_end=learning_rate/10, power=2)\n","\n","    lrs = []\n","\n","    for i in range(total):\n","        optimizer.step()\n","        lrs.append(optimizer.param_groups[0][\"lr\"])\n","        scheduler.step()\n","\n","    plt.plot(range(total), lrs)\n","\n","    print(lrs[-5:])\n","    \n","show_plot()"],"metadata":{"id":"lGz4zuf9S1Ep","execution":{"iopub.status.busy":"2022-04-25T08:09:27.541762Z","iopub.execute_input":"2022-04-25T08:09:27.542197Z","iopub.status.idle":"2022-04-25T08:09:28.141923Z","shell.execute_reply.started":"2022-04-25T08:09:27.542159Z","shell.execute_reply":"2022-04-25T08:09:28.1411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","def show_plot():\n","    optimizer = torch.optim.SGD(torch.nn.Linear(2, 1).parameters(), lr=learning_rate)\n","    total = num_epoch * len(train_loader)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps= 0, # Default value\n","                                                num_training_steps=total)\n","\n","    lrs = []\n","\n","    for i in range(total):\n","        optimizer.step()\n","        lrs.append(optimizer.param_groups[0][\"lr\"])\n","        scheduler.step()\n","\n","    plt.plot(range(total), lrs)\n","\n","    print(lrs[-5:])\n","    \n","show_plot()"],"metadata":{"id":"FaW4ajOmS1Eq","execution":{"iopub.status.busy":"2022-04-25T08:09:28.143565Z","iopub.execute_input":"2022-04-25T08:09:28.14386Z","iopub.status.idle":"2022-04-25T08:09:28.74114Z","shell.execute_reply.started":"2022-04-25T08:09:28.143821Z","shell.execute_reply":"2022-04-25T08:09:28.740441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total = num_epoch * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, 0, total)"],"metadata":{"id":"nQnepkL1S1Eq","execution":{"iopub.status.busy":"2022-04-25T08:09:28.74251Z","iopub.execute_input":"2022-04-25T08:09:28.742933Z","iopub.status.idle":"2022-04-25T08:09:28.74742Z","shell.execute_reply.started":"2022-04-25T08:09:28.742895Z","shell.execute_reply":"2022-04-25T08:09:28.746213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# if not validation:\n","#     dev_set = QA_Dataset(\"train\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n","#     train_set = torch.utils.data.ConcatDataset([train_set, dev_set])\n","#     train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n","\n","\n","if fp16_training:\n","    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n","\n","model.train()\n","\n","print(\"Start Training ...\")\n","\n","for epoch in range(num_epoch):\n","    step = 1\n","    train_loss = train_acc = 0\n","    total_step = len(train_loader) * num_epoch\n","    for data in tqdm(train_loader):    \n","        # Load all data into GPU\n","        data = [i.to(device) for i in data]\n","        \n","        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n","        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n","        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n","\n","        # Choose the most probable start position / end position\n","        start_index = torch.argmax(output.start_logits, dim=1)\n","        end_index = torch.argmax(output.end_logits, dim=1)\n","        \n","        # Prediction is correct only if both start_index and end_index are correct\n","        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n","        train_loss += output.loss\n","        \n","        if fp16_training:\n","            accelerator.backward(output.loss)\n","        else:\n","            output.loss.backward()\n","        \n","        optimizer.step()\n","        optimizer.zero_grad()\n","        step += 1\n","\n","        ##### TODO: Apply linear learning rate decay #####\n","        scheduler.step()\n","        \n","        \n","        # Print training loss and accuracy over past logging step\n","        if step % logging_step == 0:\n","            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f} | lr = {optimizer.param_groups[0]['lr']:.6f}\")\n","            train_loss = train_acc = 0\n","\n","    if validation:\n","        print(\"Evaluating Dev Set ...\")\n","        model.eval()\n","        with torch.no_grad():\n","            dev_acc = 0\n","            for i, data in enumerate(tqdm(dev_loader)):\n","                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","                       attention_mask=data[2].squeeze(dim=0).to(device))\n","                # prediction is correct only if answer text exactly matches\n","                dev_acc += evaluate(data, output, dev_set.doc_stride, \n","                                    dev_paragraphs[dev_questions[i]['paragraph_id']],\n","                                    dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].tokens,\n","                                    dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].offsets,\n","                                    ) == dev_questions[i][\"answer_text\"]\n","                \n","            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n","        model.train()\n","\n","# Save a model and its configuration file to the directory 「saved_model」 \n","# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n","# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n","print(\"Saving Model ...\")\n","model_save_dir = \"saved_model\" \n","model.save_pretrained(model_save_dir)"],"metadata":{"id":"3Q-B6ka7xoCM","execution":{"iopub.status.busy":"2022-04-25T08:09:28.74901Z","iopub.execute_input":"2022-04-25T08:09:28.749363Z","iopub.status.idle":"2022-04-25T08:09:48.882866Z","shell.execute_reply.started":"2022-04-25T08:09:28.749326Z","shell.execute_reply":"2022-04-25T08:09:48.881581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing"],"metadata":{"id":"kMmdLOKBMsdE"}},{"cell_type":"code","source":["# model = BertForQuestionAnswering.from_pretrained(\"../input/hw07tmp/saved_model\").to(device)"],"metadata":{"execution":{"iopub.status.busy":"2022-04-25T08:09:48.88439Z","iopub.status.idle":"2022-04-25T08:09:48.884856Z","shell.execute_reply.started":"2022-04-25T08:09:48.884611Z","shell.execute_reply":"2022-04-25T08:09:48.884636Z"},"trusted":true,"id":"qn-w_pN_hw4v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Evaluating Test Set ...\")\n","\n","result = []\n","\n","model.eval()\n","with torch.no_grad():\n","    for i, data in enumerate(tqdm(test_loader)):\n","        # batch size = 1 so squeeze dim=0\n","        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","                       attention_mask=data[2].squeeze(dim=0).to(device))\n","        result.append(evaluate(data, output, test_set.doc_stride, \n","                               test_paragraphs[test_questions[i]['paragraph_id']],\n","                               test_paragraphs_tokenized[test_questions[i]['paragraph_id']].tokens,\n","                               test_paragraphs_tokenized[test_questions[i]['paragraph_id']].offsets,\n","                               ))\n","\n","result_file = \"result.csv\"\n","with open(result_file, 'w') as f:    \n","      f.write(\"ID,Answer\\n\")\n","      for i, test_question in enumerate(test_questions):\n","        # Replace commas in answers with empty strings (since csv is separated by comma)\n","        # Answers in kaggle are processed in the same way\n","            f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n","\n","print(f\"Completed! Result is in {result_file}\")"],"metadata":{"id":"U5scNKC9xz0C","execution":{"iopub.status.busy":"2022-04-25T08:09:48.886366Z","iopub.status.idle":"2022-04-25T08:09:48.88699Z","shell.execute_reply.started":"2022-04-25T08:09:48.886741Z","shell.execute_reply":"2022-04-25T08:09:48.886766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# doc_stride"],"metadata":{"id":"0J2DKizPS1Et"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","x = [dev_set.doc_stride]#list(range(10, dev_set.max_paragraph_len + 10, 5))\n","y = []\n","for doc_stride in tqdm(x):\n","    dev_set.doc_stride = doc_stride\n","    model.eval()\n","    with torch.no_grad():\n","        dev_acc = 0\n","        stop_len = 10000\n","        for i, data in enumerate(tqdm(dev_loader)):\n","            if i >= stop_len:\n","                break;\n","            output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","                   attention_mask=data[2].squeeze(dim=0).to(device))\n","            # prediction is correct only if answer text exactly matches\n","            dev_acc += evaluate(data, output, dev_set.doc_stride, \n","                                dev_paragraphs[dev_questions[i]['paragraph_id']],\n","                                dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].tokens,\n","                                dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].offsets,\n","                                ) == dev_questions[i][\"answer_text\"]\n","            \n","        acc = dev_acc / min(stop_len, len(dev_loader))\n","        print(f\"{doc_stride} => acc:{acc}\")\n","        y.append(acc)\n","\n","plt.plot(x,y);\n","plt.show()"],"metadata":{"id":"1T-8vD_7S1Eu","execution":{"iopub.status.busy":"2022-04-25T08:09:48.889283Z","iopub.status.idle":"2022-04-25T08:09:48.889908Z","shell.execute_reply.started":"2022-04-25T08:09:48.889672Z","shell.execute_reply":"2022-04-25T08:09:48.889697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ensemble"],"metadata":{"id":"ltVjeLp0hw4x"}},{"cell_type":"code","source":["# diff\n","# self.max_question_len = 81; 40\n","# self.max_paragraph_len = 320; 350\n","# self.doc_stride = 270; 300\n","\n","# https://www.kaggle.com/code/zwindr/hw07-bert version:16\n","model1 = BertForQuestionAnswering.from_pretrained(\"../input/hw07tmp/saved_model\").to(device)\n","# https://www.kaggle.com/code/zwindr/pai4451-ml2021-hw7-hw7-macbert4-ipynb version:1\n","model2 = BertForQuestionAnswering.from_pretrained(\"../input/hw7-macbert4tmp/saved_model/macbert4\").to(device)\n","\n","models = [model1, model2]"],"metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:56:02.838203Z","iopub.execute_input":"2022-04-30T02:56:02.838565Z","iopub.status.idle":"2022-04-30T02:56:40.424398Z","shell.execute_reply.started":"2022-04-30T02:56:02.838528Z","shell.execute_reply":"2022-04-30T02:56:40.423666Z"},"trusted":true,"id":"VucX0PS2hw4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluateEn(data, outputs, doc_stride, paragraph, paragraph_tokenized, offsets):\n","    ##### TODO: Postprocessing #####\n","    # There is a bug and room for improvement in postprocessing \n","    # Hint: Open your prediction file to see what is wrong \n","    \n","    answer = ''\n","    max_prob = float('-inf')\n","    max_prob_bak = float('-inf')\n","    num_of_windows = data[0].shape[1]\n","    \n","    # index in the whole tokens (not just relative to window)\n","    entire_start_index = None\n","    entire_end_index = None\n","    \n","    output_start_logits = sum(output.start_logits.cpu() for output in outputs)\n","    output_end_logits = sum(output.end_logits.cpu() for output in outputs)\n","    \n","    for k in range(num_of_windows):            \n","        # print('window',k)\n","        # Obtain answer by choosing the most probable start position / end position\n","        mask = data[1][0][k].bool() & data[2][0][k].bool() # token type & attention mask\n","        masked_output_start = torch.masked_select(output_start_logits[k], mask)[:-1] # -1 is [SEP]\n","        masked_output_end = torch.masked_select(output_end_logits[k], mask)[:-1] # -1 is [SEP]\n","        \n","        start_prob, start_index = torch.max(masked_output_start, dim=0)     \n","        end_prob, end_index = torch.max(masked_output_end[start_index:], dim=0)\n","        end_index += start_index\n","        # model output means include end_index\n","        # so answer is p[start_index, end_index]\n","        \n","        # Probability of answer is calculated as sum of start_prob and end_prob\n","        prob = start_prob + end_prob\n","#         masked_data = torch.masked_select(data[0][0][k], mask)[:-1] # -1 is [SEP]\n","\n","        # Replace answer if calculated probability is larger than previous windows\n","        if (prob > max_prob) and (end_index - start_index <= 30):\n","            max_prob = prob\n","            entire_start_index = start_index.item() + doc_stride * k\n","            entire_end_index = end_index.item() + doc_stride * k\n","            # print('entire_start_index',entire_start_index)\n","            # print('entire_end_index',entire_end_index)\n","#             # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n","#             answer = tokenizer.decode(masked_data[start_index : end_index + 1])\n","\n","    answer = get_answer(paragraph, offsets, entire_start_index, entire_end_index)\n","    return answer.strip()"],"metadata":{"execution":{"iopub.status.busy":"2022-04-30T02:56:58.131399Z","iopub.execute_input":"2022-04-30T02:56:58.131712Z","iopub.status.idle":"2022-04-30T02:56:58.150555Z","shell.execute_reply.started":"2022-04-30T02:56:58.131668Z","shell.execute_reply":"2022-04-30T02:56:58.149919Z"},"trusted":true,"id":"pfkmoMhthw4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Ensemble Evaluating Test Set ...\")\n","if fp16_training:\n","    models = [accelerator.prepare(model) for model in models]\n","    \n","result = []\n","\n","for model in models:\n","    model.eval()\n","with torch.no_grad():\n","    for i, data in enumerate(tqdm(test_loader)):\n","        # batch size = 1 so squeeze dim=0\n","        outputs = [model(input_ids=data[0].squeeze(dim=0).to(device), \n","                         token_type_ids=data[1].squeeze(dim=0).to(device),\n","                           attention_mask=data[2].squeeze(dim=0).to(device)) for model in models]\n","        ans = evaluateEn(data, outputs, test_set.doc_stride, \n","                               test_paragraphs[test_questions[i]['paragraph_id']],\n","                               test_paragraphs_tokenized[test_questions[i]['paragraph_id']].tokens,\n","                               test_paragraphs_tokenized[test_questions[i]['paragraph_id']].offsets,\n","                               )\n","#         print(i, ans)\n","        result.append(ans)\n","\n","result_file = \"result.csv\"\n","with open(result_file, 'w') as f:    \n","      f.write(\"ID,Answer\\n\")\n","      for i, test_question in enumerate(test_questions):\n","        # Replace commas in answers with empty strings (since csv is separated by comma)\n","        # Answers in kaggle are processed in the same way\n","            f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n","\n","print(f\"Completed! Result is in {result_file}\")"],"metadata":{"execution":{"iopub.status.busy":"2022-04-30T03:01:13.126477Z","iopub.execute_input":"2022-04-30T03:01:13.126749Z","iopub.status.idle":"2022-04-30T03:21:22.420829Z","shell.execute_reply.started":"2022-04-30T03:01:13.126721Z","shell.execute_reply":"2022-04-30T03:21:22.419975Z"},"trusted":true,"id":"CNw65eXChw4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"sG9_jaIJhw4z"},"execution_count":null,"outputs":[]}]}