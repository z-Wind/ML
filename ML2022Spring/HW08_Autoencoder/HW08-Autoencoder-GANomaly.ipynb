{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"HW08-Autoencoder-GANomaly.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# **Homework 8 - Anomaly Detection**\n","\n","If there are any questions, please contact mlta-2022spring-ta@googlegroups.com\n","\n","Slide:    [Link]()　Kaggle: [Link](https://www.kaggle.com/c/ml2022spring-hw8)　Ref: [Link](https://github.com/pai4451/ML2021/tree/main/hw8)\n","\n","* GANomaly: \n","    * [blog](https://blog.csdn.net/qq7835144/article/details/111029750) \n","    * [github](https://github.com/qqsuhao/GANomaly-MvTec-grid)\n","    "],"metadata":{"id":"YiVfKn-6tXz8"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"qWOfbY0PWWmk","execution":{"iopub.status.busy":"2022-05-04T02:17:29.596997Z","iopub.execute_input":"2022-05-04T02:17:29.597347Z","iopub.status.idle":"2022-05-04T02:17:30.425284Z","shell.execute_reply.started":"2022-05-04T02:17:29.597229Z","shell.execute_reply":"2022-05-04T02:17:30.424276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import packages"],"metadata":{"id":"HNe7QU7n7cqh"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torchvision.models as models\n","from torch.optim import Adam, AdamW\n","\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm, trange"],"metadata":{"id":"Jk3qFK_a7k8P","execution":{"iopub.status.busy":"2022-05-04T02:17:30.427422Z","iopub.execute_input":"2022-05-04T02:17:30.428473Z","iopub.status.idle":"2022-05-04T02:17:32.02009Z","shell.execute_reply.started":"2022-05-04T02:17:30.428417Z","shell.execute_reply":"2022-05-04T02:17:32.01908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Set up the environment\n"],"metadata":{"id":"bDk9r2YOcDc9"}},{"cell_type":"markdown","source":["## Device\n","get device: cpu or cuda"],"metadata":{"id":"Hzr0Kh7eG0xA"}},{"cell_type":"code","source":["cuda = True if torch.cuda.is_available() else False\n","device = torch.device('cuda:0' if cuda else 'cpu')\n","FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","device"],"metadata":{"id":"vsQq2zfIG0xA","execution":{"iopub.status.busy":"2022-05-04T02:17:32.021282Z","iopub.execute_input":"2022-05-04T02:17:32.021491Z","iopub.status.idle":"2022-05-04T02:17:32.02988Z","shell.execute_reply.started":"2022-05-04T02:17:32.021467Z","shell.execute_reply":"2022-05-04T02:17:32.02914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Downloading data"],"metadata":{"id":"DCgNXSsEWuY7"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","if not os.path.exists('data/trainingset.npy'):\n","    !wget https://github.com/MachineLearningHW/HW8_Dataset/releases/download/v1.0.0/data.zip\n","    !unzip data.zip"],"metadata":{"id":"SCLJtgF2BLSK","execution":{"iopub.status.busy":"2022-05-04T02:17:32.031823Z","iopub.execute_input":"2022-05-04T02:17:32.032344Z","iopub.status.idle":"2022-05-04T02:17:32.06444Z","shell.execute_reply.started":"2022-05-04T02:17:32.032311Z","shell.execute_reply":"2022-05-04T02:17:32.063524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading data"],"metadata":{"id":"6X6fkGPnYyaF"}},{"cell_type":"code","source":["# # in colab\n","# train_path = 'data/trainingset.npy'\n","# test_path = 'data/testingset.npy'\n","\n","# in kaggle\n","train_path = '../input/ml2022spring-hw8/data/trainingset.npy'\n","test_path = '../input/ml2022spring-hw8/data/testingset.npy'"],"metadata":{"id":"TmZ6RwW2WWmp","execution":{"iopub.status.busy":"2022-05-04T02:17:32.066183Z","iopub.execute_input":"2022-05-04T02:17:32.066709Z","iopub.status.idle":"2022-05-04T02:17:32.070394Z","shell.execute_reply.started":"2022-05-04T02:17:32.066672Z","shell.execute_reply":"2022-05-04T02:17:32.069704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = np.load(train_path, allow_pickle=True)\n","test = np.load(test_path, allow_pickle=True)\n","\n","print(train.shape)\n","print(test.shape)"],"metadata":{"id":"k7Wd4yiUYzAm","execution":{"iopub.status.busy":"2022-05-04T02:17:32.071938Z","iopub.execute_input":"2022-05-04T02:17:32.072389Z","iopub.status.idle":"2022-05-04T02:17:46.542105Z","shell.execute_reply.started":"2022-05-04T02:17:32.072349Z","shell.execute_reply":"2022-05-04T02:17:46.540941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MEAN = torch.tensor(train.mean(axis=(0,1,2))/255)\n","# STD = torch.tensor(train.std(axis=(0,1,2))/255)\n","# MEAN = MEAN.to(device)\n","# STD = STD.to(device)\n","MEAN = torch.tensor([0.5, 0.5, 0.5]).to(device)\n","STD = torch.tensor([0.5, 0.5, 0.5]).to(device)"],"metadata":{"id":"x6SXp3a4WWmq","execution":{"iopub.status.busy":"2022-05-04T02:17:46.54383Z","iopub.execute_input":"2022-05-04T02:17:46.544176Z","iopub.status.idle":"2022-05-04T02:17:46.558114Z","shell.execute_reply.started":"2022-05-04T02:17:46.54413Z","shell.execute_reply":"2022-05-04T02:17:46.55746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show(imgs, size_inches=(15, 10)):\n","    if not isinstance(imgs, list):\n","        imgs = [imgs]\n","    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n","    fig.set_size_inches(*size_inches)\n","    for i, img in enumerate(imgs):\n","        img = torchvision.transforms.functional.to_pil_image(img)\n","        axs[0, i].imshow(np.asarray(img))\n","        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","    plt.show()"],"metadata":{"id":"CYShgxafG0w8","execution":{"iopub.status.busy":"2022-05-04T02:17:46.559629Z","iopub.execute_input":"2022-05-04T02:17:46.560482Z","iopub.status.idle":"2022-05-04T02:17:46.572462Z","shell.execute_reply.started":"2022-05-04T02:17:46.560434Z","shell.execute_reply":"2022-05-04T02:17:46.571445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_grid = torchvision.utils.make_grid([torch.tensor(train[i].transpose(2,0,1))  for i in np.random.choice(train.shape[0], 50)], nrow=10)\n","show(train_grid)\n","test_grid = torchvision.utils.make_grid([torch.tensor(test[i].transpose(2,0,1))  for i in np.random.choice(test.shape[0], 50)], nrow=10)\n","show(test_grid)"],"metadata":{"id":"_00lwDUmG0w9","execution":{"iopub.status.busy":"2022-05-04T02:17:46.57429Z","iopub.execute_input":"2022-05-04T02:17:46.57464Z","iopub.status.idle":"2022-05-04T02:17:47.501737Z","shell.execute_reply.started":"2022-05-04T02:17:46.574601Z","shell.execute_reply":"2022-05-04T02:17:47.500703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Random seed\n","Set the random seed to a certain value for reproducibility."],"metadata":{"id":"_flpmj6OYIa6"}},{"cell_type":"code","source":["def same_seeds(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","same_seeds(3)"],"metadata":{"id":"Gb-dgXQYYI2Q","execution":{"iopub.status.busy":"2022-05-04T02:17:47.504424Z","iopub.execute_input":"2022-05-04T02:17:47.504692Z","iopub.status.idle":"2022-05-04T02:17:47.51319Z","shell.execute_reply.started":"2022-05-04T02:17:47.504665Z","shell.execute_reply":"2022-05-04T02:17:47.512292Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":338},"executionInfo":{"status":"error","timestamp":1651713601862,"user_tz":-480,"elapsed":364,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}},"outputId":"043969d2-8996-4139-ea52-06bec11903cc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e7bf01a9f003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msame_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-e7bf01a9f003>\u001b[0m in \u001b[0;36msame_seeds\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msame_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"]}]},{"cell_type":"markdown","source":["# Autoencoder"],"metadata":{"id":"zR9zC0_Df-CR"}},{"cell_type":"markdown","source":["# Models & loss"],"metadata":{"id":"1EbfwRREhA7c"}},{"cell_type":"code","source":["class fcn_autoencoder(nn.Module):\n","    def __init__(self):\n","        super(fcn_autoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(64 * 64 * 3, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(), \n","            nn.Linear(64, 12), \n","            nn.ReLU(), \n","            nn.Linear(12, 3)\n","        )\n","        \n","        self.decoder = nn.Sequential(\n","            nn.Linear(3, 12),\n","            nn.ReLU(), \n","            nn.Linear(12, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 128),\n","            nn.ReLU(), \n","            nn.Linear(128, 64 * 64 * 3), \n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","    \n","    \n","\n","class conv_autoencoder(nn.Module):\n","    def __init__(self, input_channels=3):\n","        super(conv_autoencoder, self).__init__()\n","        \n","        features = 1024\n","        self.encoder = nn.Sequential(                  #  3 x 64 x 64\n","            nn.Conv2d(input_channels, 64, 4, stride=2, padding=1),  # 64 x 32 x 32\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, 4, stride=2, padding=1), # 128 x 16 x 16 \n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, 4, stride=2, padding=1), # 256 x 8 x 8\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 512, 4, stride=2, padding=1), # 512 x 4 x 4\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 1024, 4, stride=2, padding=1), # 1024 x 2 x 2\n","            nn.BatchNorm2d(1024),\n","            nn.ReLU(),\n","            nn.Conv2d(1024, features, 2, stride=1, padding=1), # features x 2 x 2\n","            nn.BatchNorm2d(1024),\n","            nn.ReLU(),\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(features, 1024, 2, stride=1, padding=1), \n","            nn.BatchNorm2d(1024),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1), \n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1), \n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), \n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), \n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, input_channels, 4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","        self.encoder = nn.Sequential(\n","            # 3 x 64 x 64\n","            nn.Conv2d(3, 12, 4, stride=2, padding=1),   \n","            nn.ReLU(),\n","            nn.Conv2d(12, 24, 4, stride=2, padding=1),\n","            nn.ReLU(),\n","        )\n","        self.enc_logvar = nn.Sequential(\n","            nn.Conv2d(24, 48, 4, stride=2, padding=1),  \n","            nn.ReLU(),\n","        )\n","        self.enc_mu = nn.Sequential(\n","            nn.Conv2d(24, 48, 4, stride=2, padding=1),\n","            nn.ReLU(),\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1), \n","            nn.ReLU(),\n","            nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1), \n","            nn.ReLU(),\n","            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1), \n","            nn.Tanh(),\n","        )\n","\n","    def encode(self, x):\n","        h1 = self.encoder(x)\n","        return self.enc_mu(h1), self.enc_logvar(h1)\n","\n","    def reparametrize(self, mu, logvar):\n","        # std = var^(0.5)\n","        # log(std) = 0.5*log(var)\n","        std = logvar.mul(0.5).exp_()\n","        eps = FloatTensor(std.size()).normal_()\n","        eps = Variable(eps)\n","        return eps.mul(std).add_(mu)\n","\n","    def decode(self, z):\n","        return self.decoder(z)\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparametrize(mu, logvar)\n","        return self.decode(z), mu, logvar\n","\n","\n","def loss_vae(recon_x, x, mu, logvar, criterion, beta):\n","    \"\"\"\n","    recon_x: generating images\n","    x: origin images\n","    mu: latent mean\n","    logvar: latent log variance\n","    \"\"\"\n","    mse = criterion(recon_x, x)\n","    # Minimize exp(logvar) - (1+logvar) + mu^2 \n","    # => ignore mu^2 => logvar = 0 make exp(logvar) always 1 would not be 0\n","    # mu^2 is L2 regularization\n","    KLD = logvar.exp() - (1+logvar) + mu.pow(2)\n","    KLD = KLD.mean(dim=0).sum()\n","#     KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n","#     KLD = torch.sum(KLD_element).mul_(-0.5)\n","#     print(\"mse\", mse, \"KLD\", KLD)\n","\n","    # less beta prevent from reconstruction loss does not decreasing\n","    return mse + beta * KLD"],"metadata":{"id":"Wi8ds1fugCkR","execution":{"iopub.status.busy":"2022-05-04T02:17:47.514488Z","iopub.execute_input":"2022-05-04T02:17:47.515103Z","iopub.status.idle":"2022-05-04T02:17:47.543648Z","shell.execute_reply.started":"2022-05-04T02:17:47.515065Z","shell.execute_reply":"2022-05-04T02:17:47.542205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = np.arange(-1, 1, 0.01)\n","y1 = np.exp(x)\n","y2 = 1+x\n","y3 = y1 - y2\n","plt.plot(x, y1, label='$e^{logvar}$')\n","plt.plot(x, y2, label='$1+logvar$')\n","plt.plot(x, y3, label='$e^{logvar} - (1+logvar)$')\n","plt.annotate('Minimum: (0,0)',\n","            xy=(0, 0), xycoords='data',\n","            xytext=(0.35, 1),\n","            arrowprops=dict(facecolor='black', shrink=0.05))\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"ADHYo0C3MFR-","execution":{"iopub.status.busy":"2022-05-04T02:17:47.545296Z","iopub.execute_input":"2022-05-04T02:17:47.546131Z","iopub.status.idle":"2022-05-04T02:17:48.178346Z","shell.execute_reply.started":"2022-05-04T02:17:47.546088Z","shell.execute_reply":"2022-05-04T02:17:48.177255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResBlock(nn.Module):\n","    def __init__(self, inchannel, outchannel, stride=1):\n","        super(ResBlock, self).__init__()\n","        #这里定义了残差块内连续的2个卷积层\n","        self.left = nn.Sequential(\n","            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n","            nn.BatchNorm2d(outchannel),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(outchannel)\n","        )\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or inchannel != outchannel:\n","            #shortcut，这里为了跟2个卷积层的结果结构一致，要做处理\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(outchannel)\n","            )\n","            \n","    def forward(self, x):\n","        out = self.left(x)\n","        #将2个卷积层的输出跟处理过的x相加，实现ResNet的基本结构\n","        out = out + self.shortcut(x)\n","        out = nn.functional.relu(out)\n","        \n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, ResBlock):\n","        super(ResNet, self).__init__()\n","        self.inchannel = 128\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU()\n","        )\n","        self.layer1 = self.make_layer(ResBlock, 256, 2, stride=2)\n","        self.layer2 = self.make_layer(ResBlock, 512, 2, stride=2)            \n","        \n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1), # [256, 8, 8]\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), # [128, 16, 16]\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), # [64, 32, 32]\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),  # [3, 64, 64]\n","            nn.Tanh(),\n","        )\n","\n","    #这个函数主要是用来，重复同一个残差块    \n","    def make_layer(self, block, channels, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.inchannel, channels, stride))\n","            self.inchannel = channels\n","        return nn.Sequential(*layers)\n","    \n","    def encode(self, x):\n","        out = self.conv1(x) # [128, 64, 64]\n","        out = self.layer1(out) # [256, 32, 32]\n","        out = self.layer2(out) # [512, 16, 16]\n","        out = nn.functional.avg_pool2d(out, 4) # [512, 4, 4]\n","        return out\n","    \n","    def decode(self, z):\n","        return self.decoder(z)\n","\n","    def forward(self, x):\n","        x = self.encode(x)\n","        x = self.decode(x)\n","        return x"],"metadata":{"id":"0wLiNEpDWWmt","execution":{"iopub.status.busy":"2022-05-04T02:17:48.180011Z","iopub.execute_input":"2022-05-04T02:17:48.18031Z","iopub.status.idle":"2022-05-04T02:17:48.197804Z","shell.execute_reply.started":"2022-05-04T02:17:48.180276Z","shell.execute_reply":"2022-05-04T02:17:48.196802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GANomaly"],"metadata":{"id":"3wwqBzBlEoQC"}},{"cell_type":"code","source":["# setting for weight init function\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","    elif classname.find('Linear') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","        m.bias.data.fill_(0)"],"metadata":{"id":"NIr4yX50EoQD","execution":{"iopub.status.busy":"2022-05-04T02:17:48.199182Z","iopub.execute_input":"2022-05-04T02:17:48.199497Z","iopub.status.idle":"2022-05-04T02:17:48.214434Z","shell.execute_reply.started":"2022-05-04T02:17:48.199457Z","shell.execute_reply":"2022-05-04T02:17:48.213557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, in_dim, feature_dim=64, out_dim=10):\n","        super(Encoder, self).__init__()\n","        self.encorder = self.new_encorder(in_dim, feature_dim, out_dim)\n","    \n","    def new_encorder(self, in_dim, feature_dim, out_dim):\n","        return nn.Sequential(       \n","            nn.AvgPool2d(5, stride=1, padding=2),\n","            nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), # (batch, 64, 32, 32)\n","            nn.LeakyReLU(0.2),\n","            self.conv_bn_lrelu(feature_dim, feature_dim * 2),                   # (batch, 128, 16, 16)\n","            self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4),               # (batch, 256, 8, 8)\n","            self.conv_bn_lrelu(feature_dim * 4, out_dim),                       # (batch, 10, 4, 4)\n","        )\n","    \n","    def conv_bn_lrelu(self, in_dim, out_dim):\n","        return nn.Sequential(\n","            nn.Conv2d(in_dim, out_dim, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(out_dim),\n","            nn.LeakyReLU(0.2),\n","        )\n","    \n","    def forward(self, x):\n","        x = self.encorder(x)\n","        return x\n","    \n","class Generator(nn.Module):\n","    def __init__(self, in_dim, feature_dim=64, out_dim=10):\n","        super(Generator, self).__init__()\n","        \n","        #input: (batch, 3, 64, 64)\n","        self.encoder1 = Encoder(in_dim, feature_dim, out_dim)\n","        self.encoder2 = Encoder(in_dim, feature_dim, out_dim)\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(out_dim, feature_dim * 4, 4, stride=2, padding=1),    # (batch, 256, 8, 8)\n","            nn.ReLU(), \n","            self.convTrans_bn_lrelu(feature_dim * 4, feature_dim * 2),               # (batch, 128, 16, 16)\n","            self.convTrans_bn_lrelu(feature_dim * 2, feature_dim),                   # (batch, 64, 32, 32)\n","            nn.ConvTranspose2d(feature_dim, in_dim, 4, stride=2, padding=1),         # (batch, 3, 64, 64)\n","            nn.Tanh(),\n","        )\n","#         self.apply(weights_init)\n","        \n","    \n","    def convTrans_bn_lrelu(self, in_dim, out_dim):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(in_dim, out_dim, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(out_dim),\n","            nn.LeakyReLU(0.2),\n","        )\n","        \n","    def forward(self, x):\n","        f1 = self.encoder1(x)\n","        x = self.decoder(f1)\n","        f2 = self.encoder2(x)\n","        return x, f1, f2\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, in_dim, feature_dim=64, out_dim=10):\n","        super(Discriminator, self).__init__()\n","            \n","        #input: (batch, 3, 64, 64)\n","        self.encoder = Encoder(in_dim, feature_dim, out_dim)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(out_dim*4*4, 1),\n","            nn.Sigmoid(),\n","        )\n","#         self.apply(weights_init)\n","    \n","    def forward(self, x):\n","        features = self.encoder(x)\n","        y = features.view(x.shape[0], -1)\n","        y = self.classifier(y)\n","        y = y.squeeze(1) # (B, 1) -> (B)\n","        return y, features\n","\n","def loss_GAN(rec_loss, feature_loss, adv_loss):    \n","    return 10 * rec_loss + feature_loss + adv_loss "],"metadata":{"id":"M2oa-yyoEoQD","execution":{"iopub.status.busy":"2022-05-04T02:17:48.215743Z","iopub.execute_input":"2022-05-04T02:17:48.216187Z","iopub.status.idle":"2022-05-04T02:17:48.232984Z","shell.execute_reply.started":"2022-05-04T02:17:48.216139Z","shell.execute_reply":"2022-05-04T02:17:48.232189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset module\n","\n","Module for obtaining and processing data. The transform function here normalizes image's pixels from [0, 255] to [-1.0, 1.0].\n"],"metadata":{"id":"vrJ9bScg9AgO"}},{"cell_type":"code","source":["class CustomTensorDataset(TensorDataset):\n","    \"\"\"TensorDataset with support of transforms.\n","    \"\"\"\n","    def __init__(self, tensors, half=False):\n","        self.half = half\n","        self.tensors = tensors\n","        if tensors.shape[-1] == 3:\n","            self.tensors = tensors.permute(0, 3, 1, 2)\n","        \n","        self.transform = transforms.Compose([\n","            transforms.Lambda(lambda x: x.to(torch.float32)),\n","            transforms.Lambda(lambda x: 2. * x/255. - 1.),\n","#             transforms.ToPILImage(),\n","#             transforms.ToTensor(), # range [0, 255] -> [0.0, 1.0]\n","#             # output[channel] = (input[channel] - mean[channel]) / std[channel]\n","#             transforms.Normalize(mean=MEAN, std=STD), # range [0, 1.0] -> [-1.0, 1.0]\n","#             transforms.Grayscale(),\n","        ])\n","        \n","    def __getitem__(self, index):\n","        x = self.tensors[index]\n","        \n","        if self.transform:\n","            # mapping images to [-1.0, 1.0]\n","            x = self.transform(x)\n","        \n","        if self.half:\n","            height_cutoff = x.shape[1] // 2 # for detect hat\n","            x = x[:, :height_cutoff, :]\n","        return x\n","\n","    def __len__(self):\n","        return len(self.tensors)\n","\n","    \n","batch_size = 128 # smaller batchsize is better\n","# Build training dataloader\n","train_dataset = CustomTensorDataset(torch.from_numpy(train))\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n","\n","eval_batch_size = 200\n","# build testing dataloader\n","test_dataset = CustomTensorDataset(torch.from_numpy(test))\n","test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=eval_batch_size)"],"metadata":{"id":"33fWhE-h9LPq","execution":{"iopub.status.busy":"2022-05-04T02:17:48.234671Z","iopub.execute_input":"2022-05-04T02:17:48.235648Z","iopub.status.idle":"2022-05-04T02:17:48.254234Z","shell.execute_reply.started":"2022-05-04T02:17:48.2356Z","shell.execute_reply":"2022-05-04T02:17:48.253292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recover_from_normalize = lambda img: img * STD[:, None, None] + MEAN[:, None, None]\n","\n","imgs = iter(train_dataloader).next()[:10]\n","train_grid = torchvision.utils.make_grid([recover_from_normalize(img.to(device)) for img in imgs], nrow=5)\n","show(train_grid, size_inches=(15, 5))"],"metadata":{"id":"CcyL2BQ4G0xK","execution":{"iopub.status.busy":"2022-05-04T02:17:48.255721Z","iopub.execute_input":"2022-05-04T02:17:48.256401Z","iopub.status.idle":"2022-05-04T02:17:48.6023Z","shell.execute_reply.started":"2022-05-04T02:17:48.256357Z","shell.execute_reply":"2022-05-04T02:17:48.601336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"XKNUImqUhIeq"}},{"cell_type":"markdown","source":["## Configuration\n"],"metadata":{"id":"7ebAJdjFmS08"}},{"cell_type":"code","source":["# Training hyperparameters\n","num_epochs = 2\n","learning_rate = 1e-3"],"metadata":{"id":"in7yLfmqtZTk","execution":{"iopub.status.busy":"2022-05-04T02:17:48.603658Z","iopub.execute_input":"2022-05-04T02:17:48.603907Z","iopub.status.idle":"2022-05-04T02:17:48.608345Z","shell.execute_reply.started":"2022-05-04T02:17:48.603878Z","shell.execute_reply":"2022-05-04T02:17:48.607493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training loop"],"metadata":{"id":"wyooN-JPm8sS"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","# Model\n","model_type = 'fcn'   # selecting a model type from {'cnn', 'fcn', 'vae', 'resnet'}\n","model_classes = {'fcn': fcn_autoencoder(), 'cnn': conv_autoencoder(3), 'vae': VAE(), 'resnet': ResNet(ResBlock)}\n","model = model_classes[model_type].to(device)"],"metadata":{"id":"ftUZGz0VEoQJ","execution":{"iopub.status.busy":"2022-05-04T02:17:48.609753Z","iopub.execute_input":"2022-05-04T02:17:48.610309Z","iopub.status.idle":"2022-05-04T02:17:48.645654Z","shell.execute_reply.started":"2022-05-04T02:17:48.610255Z","shell.execute_reply":"2022-05-04T02:17:48.644521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","# Loss and optimizer\n","criterion = nn.MSELoss(reduction='mean')\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","best_loss = np.inf\n","model.train()\n","\n","for epoch in range(num_epochs):\n","    tot_loss = list()\n","    progress_bar = tqdm(train_dataloader)\n","    for data in progress_bar:\n","        # ===================loading=====================\n","        img = data.float().to(device)\n","        if model_type in ['fcn']:\n","            img = img.view(img.shape[0], -1)\n","\n","        # ===================forward=====================\n","        output = model(img)\n","        if model_type in ['vae']:\n","            loss = loss_vae(output[0], img, output[1], output[2], criterion, epoch/100000)\n","#             loss = criterion(output[0], img)\n","        else:\n","            loss = criterion(output, img)\n","\n","        tot_loss.append(loss.item())\n","        # ===================backward====================\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # ===================log========================\n","        progress_bar.set_postfix(cur_loss=loss.item())\n","        \n","    # ===================save_best====================\n","    mean_loss = np.mean(tot_loss)\n","    if mean_loss < best_loss:\n","        print(f\"epoch: {epoch + 1:3d}, best loss: {mean_loss:.4f}\")\n","        if model_type in ['vae', 'ganomaly']:\n","            ori = torchvision.utils.make_grid([recover_from_normalize(x) for x in img[:5]], nrow=5)\n","            rec = torchvision.utils.make_grid([recover_from_normalize(x) for x in output[0][:5]], nrow=5)\n","        elif model_type in [\"fcn\"]:\n","            ori = torchvision.utils.make_grid([recover_from_normalize(x.view(3,64,64)) for x in img[:5]], nrow=5) \n","            rec = torchvision.utils.make_grid([recover_from_normalize(x.view(3,64,64)) for x in output[:5]], nrow=5)\n","        else:\n","            ori = torchvision.utils.make_grid([recover_from_normalize(x) for x in img[:5]], nrow=5) \n","            rec = torchvision.utils.make_grid([recover_from_normalize(x) for x in output[:5]], nrow=5)\n","        \n","        diff = (ori - rec).abs()\n","        show(ori, size_inches=(15, 5))\n","        show(rec, size_inches=(15, 5))\n","        show(diff, size_inches=(15, 5))\n","        \n","        best_loss = mean_loss\n","        torch.save(model, 'best_model_{}.pt'.format(model_type))\n","    # ===================log========================\n","    print(f\"epoch: {epoch + 1:3d}, loss: {mean_loss:.4f}\")\n","    # ===================save_last========================\n","    torch.save(model, 'last_model_{}.pt'.format(model_type))"],"metadata":{"id":"JoW1UrrxgI_U","execution":{"iopub.status.busy":"2022-05-04T02:17:48.647932Z","iopub.execute_input":"2022-05-04T02:17:48.648216Z","iopub.status.idle":"2022-05-04T02:17:48.672596Z","shell.execute_reply.started":"2022-05-04T02:17:48.648187Z","shell.execute_reply":"2022-05-04T02:17:48.671423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_type = \"ganomaly\"\n","# D.csv: 0.77339, DD.csv: 0.73842 feature_dim = 256\n","# D.csv: 0.80093, DD.csv: 0.82131 feature_dim = 128\n","# D.csv: 0.75548, DD.csv: 0.85475 feature_dim = 64\n","# D.csv: 0.75815, DD.csv: 0.82512 feature_dim = 32\n","feature_dim = 64\n","out_dim = 100\n","# Model\n","G = Generator(3, feature_dim, out_dim).to(device)\n","D = Discriminator(3).to(device)\n","G.apply(weights_init)\n","D.apply(weights_init)\n","print(G)\n","print(D)"],"metadata":{"id":"ZgtDBp1UEoQL","execution":{"iopub.status.busy":"2022-05-04T02:17:48.674417Z","iopub.execute_input":"2022-05-04T02:17:48.675415Z","iopub.status.idle":"2022-05-04T02:17:48.767628Z","shell.execute_reply.started":"2022-05-04T02:17:48.675371Z","shell.execute_reply":"2022-05-04T02:17:48.766661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# img gen\n","z_dim = (out_dim, 4, 4)\n","\n","# Loss and optimizer\n","criterion_D = nn.BCELoss()\n","criterion_G = nn.MSELoss()\n","opt_D = torch.optim.Adam(D.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","opt_G = torch.optim.Adam(G.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","\n","best_loss = np.array([np.inf, np.inf, np.inf])\n","G.train()\n","D.train()\n","\n","for epoch in range(num_epochs):\n","    tot_loss = list()\n","    progress_bar = tqdm(train_dataloader)\n","    progress_bar.set_description(f\"Epoch {epoch+1}\")\n","    for data in progress_bar:\n","        # ===================loading=====================\n","        imgs = data.float().to(device)\n","        bs = imgs.size(0)\n","        \n","        # *********************\n","        # *    Train D        *\n","        # *********************\n","        z = Variable(torch.randn(bs, *z_dim)).to(device)\n","#         r_imgs, _, _ = G(imgs) => bad\n","        r_imgs = Variable(imgs).to(device)\n","        f_imgs = G.decoder(z)\n","        r_label = torch.ones((bs)).to(device)\n","        f_label = torch.zeros((bs)).to(device)\n","\n","        # Discriminator forwarding\n","        r_logit, _ = D(r_imgs)\n","        f_logit, _ = D(f_imgs)\n","\n","        # Loss for discriminator\n","        r_loss = criterion_D(r_logit, r_label)\n","        f_loss = criterion_D(f_logit, f_label)\n","        loss_D = (r_loss + f_loss) / 2\n","        \n","        # Discriminator backwarding\n","        D.zero_grad()\n","        loss_D.backward()\n","        opt_D.step()\n","        \n","        # *********************\n","        # *    Train G        *\n","        # *********************\n","        rec_imgs, feature_in, feature_out = G(imgs)\n","        _, feature_r_D = D(imgs)\n","        rec_logit, feature_f_D = D(rec_imgs)\n","        rec_label = torch.ones((bs)).to(device)\n","        \n","        loss_G_rec = criterion_G(rec_imgs, imgs)\n","        loss_G_fea = criterion_G(feature_in, feature_out)\n","        loss_G_adv = criterion_G(feature_r_D, feature_f_D) \n","#         loss_G_adv = criterion_D(rec_logit, rec_label) => bad\n","        loss_G = loss_GAN(loss_G_rec, loss_G_fea, loss_G_adv)\n","        \n","        # Generator backwarding\n","        G.zero_grad()\n","        loss_G.backward()\n","        opt_G.step()\n","\n","        # ===================log========================\n","        tot_loss.append((loss_D.item(), loss_G.item()))\n","        progress_bar.set_postfix(loss_D=loss_D.item(), \n","                    loss_G=loss_G.item(), \n","                    loss_G_rec=loss_G_rec.item(), \n","                    loss_G_fea=loss_G_fea.item(), \n","                    loss_G_adv=loss_G_adv.item())\n","        \n","    # ===================save_best====================    \n","    mean_loss = np.mean(tot_loss, axis=0)\n","    if mean_loss.sum() < best_loss.sum():\n","        print(f\"epoch: {epoch + 1:3d}, best loss: {mean_loss}\")\n","\n","        ori = torchvision.utils.make_grid([recover_from_normalize(x) for x in imgs[:5]], nrow=5)\n","        rec = torchvision.utils.make_grid([recover_from_normalize(x) for x in rec_imgs[:5]], nrow=5)\n","        diff = (ori - rec).abs()\n","        gen_rand = torchvision.utils.make_grid([recover_from_normalize(x) for x in f_imgs[:5]], nrow=5)\n","\n","        show(ori, size_inches=(15, 5))\n","        show(rec, size_inches=(15, 5))\n","        show(diff, size_inches=(15, 5))\n","        show(gen_rand, size_inches=(15, 5))\n","\n","        best_loss = mean_loss\n","        torch.save(G, 'best_G_model.pt')\n","        torch.save(D, 'best_D_model.pt')\n","    # ===================log========================\n","    print(f\"epoch: {epoch + 1:3d}, loss: {mean_loss}\")\n","    # ===================save_last========================\n","    torch.save(D, 'last_model_D.pt')\n","    torch.save(G, 'last_model_G.pt')"],"metadata":{"id":"nmfCMuyyEoQL","execution":{"iopub.status.busy":"2022-05-04T02:17:48.769083Z","iopub.execute_input":"2022-05-04T02:17:48.769746Z","iopub.status.idle":"2022-05-04T05:29:22.183657Z","shell.execute_reply.started":"2022-05-04T02:17:48.76971Z","shell.execute_reply":"2022-05-04T05:29:22.181702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference\n","Model is loaded and generates its anomaly score predictions."],"metadata":{"id":"Wk0UxFuchLzR"}},{"cell_type":"markdown","source":["## Initialize\n","- dataloader\n","- model\n","- prediction file"],"metadata":{"id":"evgMW3OwoGqD"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","# load trained model\n","checkpoint_path = f'last_model_{model_type}.pt'\n","model = torch.load(checkpoint_path)"],"metadata":{"id":"_MBnXAswoKmq","execution":{"iopub.status.busy":"2022-05-04T05:29:22.188806Z","iopub.execute_input":"2022-05-04T05:29:22.18916Z","iopub.status.idle":"2022-05-04T05:29:22.282427Z","shell.execute_reply.started":"2022-05-04T05:29:22.189109Z","shell.execute_reply":"2022-05-04T05:29:22.281436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","model.eval()\n","eval_loss = nn.MSELoss(reduction='none')\n","\n","# prediction file \n","out_file = 'prediction.csv'\n","\n","max_loss = -np.inf\n","\n","anomality = list()\n","with torch.no_grad():\n","    for i, data in enumerate(tqdm(test_dataloader)):\n","        img = data.float().to(device)\n","        if model_type in ['fcn']:\n","            img = img.view(img.shape[0], -1)\n","\n","        output = model(img)\n","        if model_type in ['vae', 'ganomaly']:\n","            output = output[0]\n","            \n","#         height_cutoff = output.shape[1] // 2 # for detect hat\n","#         output = output[:, :height_cutoff, :]\n","#         img = img[:, :height_cutoff, :]\n","        if model_type in ['fcn']:\n","            loss = eval_loss(output, img).sum(-1)\n","        else:\n","            loss = eval_loss(output, img).sum([1, 2, 3])\n","        anomality.append(loss)\n","\n","        val, i = torch.max(loss, dim=0)\n","        if val > max_loss:\n","            max_loss = val\n","            if model_type in ['fcn']:\n","                ori = recover_from_normalize(img[i].view(3,64,64))\n","                rec = recover_from_normalize(output[i].view(3,64,64))\n","            else:\n","                ori = recover_from_normalize(img[i])\n","                rec = recover_from_normalize(output[i])\n","\n","            diff = (ori - rec).abs()\n","            show(torchvision.utils.make_grid([ori, rec, diff]), size_inches=(6, 6))\n","\n","anomality = torch.cat(anomality, axis=0)\n","anomality = torch.sqrt(anomality).reshape(len(test), 1).cpu().numpy()\n","\n","df = pd.DataFrame(anomality, columns=['score'])\n","df.to_csv(out_file, index_label = 'ID')"],"metadata":{"id":"_1IxCX2iCW6V","execution":{"iopub.status.busy":"2022-05-04T05:29:22.286662Z","iopub.execute_input":"2022-05-04T05:29:22.286945Z","iopub.status.idle":"2022-05-04T05:29:22.344469Z","shell.execute_reply.started":"2022-05-04T05:29:22.286916Z","shell.execute_reply":"2022-05-04T05:29:22.343291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # load trained model\n","# G = torch.load(\"../input/hw08tmp/last_model_G.pt\", map_location=device)\n","# D = torch.load(\"../input/hw08tmp/last_model_D.pt\", map_location=device)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-04T05:29:22.346724Z","iopub.execute_input":"2022-05-04T05:29:22.347083Z","iopub.status.idle":"2022-05-04T05:29:22.351734Z","shell.execute_reply.started":"2022-05-04T05:29:22.347037Z","shell.execute_reply":"2022-05-04T05:29:22.350858Z"},"trusted":true,"id":"zInscbmmy8tm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G.eval()\n","D.eval()\n","\n","# prediction file \n","loss_fn = nn.MSELoss(reduction=\"none\")\n","\n","anomality_D = list()\n","anomality_rec = list()\n","anomality_F = list()\n","anomality_FF = list()\n","with torch.no_grad():\n","    for i, data in enumerate(tqdm(test_dataloader)):\n","        img = data.float().to(device)\n","        rec_img, f1, f2 = G(img)\n","        _, f3, _ = G(rec_img)\n","        y, _ = D(img)\n","\n","        loss_D = -y\n","        anomality_D.append(loss_D)\n","\n","        loss_rec = loss_fn(img, rec_img).mean(dim=(1,2,3))\n","        anomality_rec.append(loss_rec)\n","\n","        loss_F = loss_fn(f1, f2).mean(dim=(1,2,3))\n","        anomality_F.append(loss_F)\n","\n","        loss_FF = loss_fn(f1, f3).mean(dim=(1,2,3))\n","        anomality_FF.append(loss_FF)\n","\n","def process_anomality(anomality, outputfile):\n","    anomality = torch.cat(anomality, axis=0)\n","    anomality = anomality.reshape(len(test), 1).cpu().numpy()\n","    df = pd.DataFrame(anomality, columns=['score'])\n","    df.to_csv(outputfile, index_label = 'ID')\n","    df.plot.hist(bins=100, title=outputfile)\n","\n","    return df\n","\n","df_D = process_anomality(anomality_D, 'prediction_D.csv')\n","df_rec = process_anomality(anomality_rec, 'prediction_rec.csv')\n","df_F = process_anomality(anomality_F, 'prediction_F.csv')\n","df_FF = process_anomality(anomality_FF, 'prediction_FF.csv')"],"metadata":{"id":"E8GLJI24EoQQ","execution":{"iopub.status.busy":"2022-05-04T05:29:22.353352Z","iopub.execute_input":"2022-05-04T05:29:22.353649Z","iopub.status.idle":"2022-05-04T05:33:53.617909Z","shell.execute_reply.started":"2022-05-04T05:29:22.353607Z","shell.execute_reply":"2022-05-04T05:33:53.617044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" def show_cmp(df, num=3*20):\n","    df_sort = df.sort_values('score').reset_index()\n","    normal = df_sort[:num]\n","    abnormal = df_sort[-num:]\n","\n","    normal_img = []\n","    abnormal_img = []\n","    with torch.no_grad():\n","        for i, data in enumerate(tqdm(test_dataset)): \n","            if (normal['index'] == i).any():\n","                normal_img.append(recover_from_normalize(data.to(device)))\n","            elif (abnormal['index'] == i).any():\n","                abnormal_img.append(recover_from_normalize(data.to(device)))\n","                \n","    print(\"normal\")\n","    show(torchvision.utils.make_grid(normal_img, nrow=20), size_inches=(18, 18))\n","    print(\"abnormal\")\n","    show(torchvision.utils.make_grid(abnormal_img, nrow=20), size_inches=(18, 18))\n","\n","for df in [df_D, df_rec, df_F, df_FF]:\n","    show_cmp(df)"],"metadata":{"id":"MWC1IZSeEoQR","execution":{"iopub.status.busy":"2022-05-04T05:35:27.839771Z","iopub.execute_input":"2022-05-04T05:35:27.840447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## More Train"],"metadata":{"id":"jV290EJ4y8tp"}},{"cell_type":"code","source":["epochs = 1\n","df_sort = df_D.sort_values('score').reset_index()\n","normal = df_sort[:50*10]\n","abnormal = df_sort[-50*10:]\n","    \n","# Loss and optimizer\n","criterion_D = nn.BCELoss()\n","opt_D = torch.optim.Adam(D.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","\n","D.train()\n","\n","for epoch in range(epochs):\n","    progress_bar = tqdm(test_dataset)\n","    progress_bar.set_description(f\"Epoch {epoch+1}\")\n","    for i, data in enumerate(progress_bar):\n","        label = None\n","        if (normal['index'] == i).any():\n","            label = torch.ones((1,)).to(device)\n","        elif (abnormal['index'] == i).any():\n","            label = torch.zeros((1,)).to(device)\n","        else:\n","            continue\n","\n","        # ===================loading=====================\n","        imgs = data.float().to(device).unsqueeze(0)\n","\n","        # *********************\n","        # *    Train D        *\n","        # *********************\n","\n","        # Discriminator forwarding\n","        logit, _ = D(imgs)\n","\n","        # Loss for discriminator\n","        loss_D = criterion_D(logit, label)\n","\n","        # Discriminator backwarding\n","        D.zero_grad()\n","        loss_D.backward()\n","        opt_D.step()\n","\n","        # ===================log========================\n","        progress_bar.set_postfix(loss_D=loss_D.item())\n","        "],"metadata":{"execution":{"iopub.status.busy":"2022-05-04T05:34:25.00096Z","iopub.execute_input":"2022-05-04T05:34:25.001234Z","iopub.status.idle":"2022-05-04T05:34:48.335703Z","shell.execute_reply.started":"2022-05-04T05:34:25.001203Z","shell.execute_reply":"2022-05-04T05:34:48.334892Z"},"trusted":true,"id":"PmfWUGury8tq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G.eval()\n","D.eval()\n","\n","# prediction file \n","loss_fn = nn.MSELoss(reduction=\"none\")\n","\n","anomality_DD = list()\n","with torch.no_grad():\n","    for i, data in enumerate(tqdm(test_dataloader)):\n","        img = data.float().to(device)\n","        y, _ = D(img)\n","\n","        loss_DD = -y\n","        anomality_DD.append(loss_DD)\n","\n","df_DD = process_anomality(anomality_DD, 'prediction_DD.csv')\n","show_cmp(df_DD, 50*10)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-04T05:34:48.336842Z","iopub.execute_input":"2022-05-04T05:34:48.337061Z","iopub.status.idle":"2022-05-04T05:35:27.825416Z","shell.execute_reply.started":"2022-05-04T05:34:48.337034Z","shell.execute_reply":"2022-05-04T05:35:27.824316Z"},"trusted":true,"id":"X74IxcpBy8tr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Report"],"metadata":{"id":"Evil-wwZWWmx"}},{"cell_type":"markdown","source":["2. Train a fully connected autoencoder and adjust at least two different\n","element of the latent representation. Show your model architecture, plot\n","out the original image, the reconstructed images for each adjustment and\n","describe the differences."],"metadata":{"id":"YKHsQ8q0WWmy"}},{"cell_type":"code","source":["if model_type in ['fcn']:\n","    show(torchvision.utils.make_grid([recover_from_normalize(model.decoder(FloatTensor([x, 0, 0])).view(3,64,64)) for x in range(-20,20)]))\n","    show(torchvision.utils.make_grid([recover_from_normalize(model.decoder(FloatTensor([0, x, 0])).view(3,64,64)) for x in range(-20,20)]))\n","    show(torchvision.utils.make_grid([recover_from_normalize(model.decoder(FloatTensor([0, 0, x])).view(3,64,64)) for x in range(-20,20)]))"],"metadata":{"id":"81y89slEG0xR","execution":{"iopub.status.busy":"2022-05-04T05:35:27.827279Z","iopub.execute_input":"2022-05-04T05:35:27.827616Z","iopub.status.idle":"2022-05-04T05:35:27.837581Z","shell.execute_reply.started":"2022-05-04T05:35:27.827567Z","shell.execute_reply":"2022-05-04T05:35:27.836329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"f9QfvJyLWWmy"},"execution_count":null,"outputs":[]}]}