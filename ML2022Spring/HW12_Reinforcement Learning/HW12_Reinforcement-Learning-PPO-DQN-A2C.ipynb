{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"HW12_Reinforcement-Learning-PPO-DQN-A2C.ipynb","provenance":[],"collapsed_sections":["ezdfoThbAQ49","GKdS8vOihxhc","Mhqp6D-XgHpe","jQUbWcWAJD3A","vRGtz56oJW8M","ouv23glgf5Qt","vNb_tuFYhKVK","mXyz8pq9JD3R","--yXbCe4JD3T","ipO5IO7kJD3e"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# **Homework 12 - Reinforcement Learning**\n","\n","If you have any problem, e-mail us at ntu-ml-2022spring-ta@googlegroups.com\n","\n"],"metadata":{"id":"Fp30SB4bxeQb"}},{"cell_type":"markdown","source":["* [Rainbow:整合DQN六種改進的深度強化學習方法！](https://www.jianshu.com/p/1dfd84cd2e69)\n","* [ericyangyu/PPO-for-Beginners](https://github.com/ericyangyu/PPO-for-Beginners)"],"metadata":{"id":"EGVTzTDtJD2R"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:41.135310Z","iopub.execute_input":"2022-06-01T02:32:41.135532Z","iopub.status.idle":"2022-06-01T02:32:41.423275Z","shell.execute_reply.started":"2022-06-01T02:32:41.135499Z","shell.execute_reply":"2022-06-01T02:32:41.422382Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"mZ8IxH-xJD2T","executionInfo":{"status":"ok","timestamp":1654051934824,"user_tz":-480,"elapsed":34,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}},"outputId":"00643033-33ba-466e-e535-d75bd036abde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"markdown","source":["## Preliminary work\n","\n","First, we need to install all necessary packages.\n","One of them, gym, builded by OpenAI, is a toolkit for developing Reinforcement Learning algorithm. Other packages are for visualization in colab."],"metadata":{"id":"yXsnCWPtWSNk"}},{"cell_type":"code","source":["!apt update\n","!apt install python-opengl xvfb -y\n","!pip install gym[box2d]==0.18.3 pyvirtualdisplay numpy==1.19.5 torch==1.8.1"],"metadata":{"id":"5e2bScpnkVbv","outputId":"c104706f-4c11-4090-c4ca-31999335df8b","execution":{"iopub.status.busy":"2022-06-01T02:32:41.424555Z","iopub.execute_input":"2022-06-01T02:32:41.424777Z","iopub.status.idle":"2022-06-01T02:32:56.717823Z","shell.execute_reply.started":"2022-06-01T02:32:41.424749Z","shell.execute_reply":"2022-06-01T02:32:56.716934Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654051950016,"user_tz":-480,"elapsed":15208,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\u001b[33m\r0% [Connecting to security.ubuntu.com (91.189.91.39)] [Connected to cloud.r-pro\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","\r                                                                               \rHit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","\u001b[33m\r0% [Waiting for headers] [Connected to cloud.r-project.org (13.225.213.82)] [Co\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connected to cloud.r-projec\u001b[0m\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","77 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","python-opengl is already the newest version (3.1.0+dfsg-1).\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 77 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[box2d]==0.18.3 in /usr/local/lib/python3.7/dist-packages (0.18.3)\n","Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n","Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (1.8.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (4.2.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.18.3) (1.4.1)\n","Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.18.3) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.18.3) (1.3.0)\n","Requirement already satisfied: Pillow<=8.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.18.3) (7.1.2)\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.18.3) (2.3.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.15,>=1.4.0->gym[box2d]==0.18.3) (0.16.0)\n"]}]},{"cell_type":"markdown","source":["\n","Next, set up virtual display，and import all necessaary packages."],"metadata":{"id":"M_-i3cdoYsks"}},{"cell_type":"code","source":["%%capture\n","from pyvirtualdisplay import Display\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","from IPython import display\n","\n","import math\n","import numpy as np\n","from tqdm.auto import tqdm\n","from collections import deque\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd \n","import torch.nn.functional as F\n","from torch.distributions import Categorical"],"metadata":{"id":"nl2nREINDLiw","execution":{"iopub.status.busy":"2022-06-01T02:32:56.721547Z","iopub.execute_input":"2022-06-01T02:32:56.721852Z","iopub.status.idle":"2022-06-01T02:32:56.732197Z","shell.execute_reply.started":"2022-06-01T02:32:56.721821Z","shell.execute_reply":"2022-06-01T02:32:56.731541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cuda = True if torch.cuda.is_available() else False\n","device = torch.device('cuda:0' if cuda else 'cpu')\n","FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if cuda else autograd.Variable(*args, **kwargs)\n","device"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:56.733492Z","iopub.execute_input":"2022-06-01T02:32:56.733649Z","iopub.status.idle":"2022-06-01T02:32:56.747401Z","shell.execute_reply.started":"2022-06-01T02:32:56.733628Z","shell.execute_reply":"2022-06-01T02:32:56.746943Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Kbws0T1BJD2g","executionInfo":{"status":"ok","timestamp":1654051950019,"user_tz":-480,"elapsed":66,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}},"outputId":"960204cf-cd2a-43ff-d737-02164867ecc0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["# Warning ! Do not revise random seed !!!\n","# Your submission on JudgeBoi will not reproduce your result !!!\n","Make your HW result to be reproducible.\n"],"metadata":{"id":"CaEJ8BUCpN9P"}},{"cell_type":"code","source":["seed = 543 # Do not change this\n","def fix(env, seed):\n","    env.seed(seed)\n","    env.action_space.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.set_deterministic(True)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True"],"metadata":{"id":"fV9i8i2YkRbO","execution":{"iopub.status.busy":"2022-06-01T02:32:56.748086Z","iopub.execute_input":"2022-06-01T02:32:56.748349Z","iopub.status.idle":"2022-06-01T02:32:56.764991Z","shell.execute_reply.started":"2022-06-01T02:32:56.748323Z","shell.execute_reply":"2022-06-01T02:32:56.764159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Last, call gym and build an [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment."],"metadata":{"id":"He0XDx6bzjgC"}},{"cell_type":"code","source":["%%capture\n","import gym\n","import random\n","env = gym.make('LunarLander-v2')\n","fix(env, seed) # fix the environment Do not revise this !!!"],"metadata":{"id":"N_4-xJcbBt09","execution":{"iopub.status.busy":"2022-06-01T02:32:56.766530Z","iopub.execute_input":"2022-06-01T02:32:56.767337Z","iopub.status.idle":"2022-06-01T02:32:56.784330Z","shell.execute_reply.started":"2022-06-01T02:32:56.767292Z","shell.execute_reply":"2022-06-01T02:32:56.783496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## What Lunar Lander？\n","\n","“LunarLander-v2”is to simulate the situation when the craft lands on the surface of the moon.\n","\n","This task is to enable the craft to land \"safely\" at the pad between the two yellow flags.\n","> Landing pad is always at coordinates (0,0).\n","> Coordinates are the first two numbers in state vector.\n","\n","![](https://user-images.githubusercontent.com/15806078/153222406-af5ce6f0-4696-4a24-a683-46ad4939170c.gif)\n","\n","\"LunarLander-v2\" actually includes \"Agent\" and \"Environment\". \n","\n","In this homework, we will utilize the function `step()` to control the action of \"Agent\". \n","\n","Then `step()` will return the observation/state and reward given by the \"Environment\"."],"metadata":{"id":"NrkVvTrvWZ5H"}},{"cell_type":"markdown","source":["### Observation / State\n","\n","First, we can take a look at what an Observation / State looks like."],"metadata":{"id":"bIbp82sljvAt"}},{"cell_type":"code","source":["print(env.observation_space)"],"metadata":{"id":"rsXZra3N9R5T","outputId":"35df041a-45d4-46cc-ab0a-4df938080bd6","execution":{"iopub.status.busy":"2022-06-01T02:32:56.785258Z","iopub.execute_input":"2022-06-01T02:32:56.785712Z","iopub.status.idle":"2022-06-01T02:32:56.798524Z","shell.execute_reply.started":"2022-06-01T02:32:56.785680Z","shell.execute_reply":"2022-06-01T02:32:56.797873Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654051950024,"user_tz":-480,"elapsed":57,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Box(-inf, inf, (8,), float32)\n"]}]},{"cell_type":"markdown","source":["\n","`Box(8,)`means that observation is an 8-dim vector\n","### Action\n","\n","Actions can be taken by looks like"],"metadata":{"id":"ezdfoThbAQ49"}},{"cell_type":"code","source":["print(env.action_space)"],"metadata":{"id":"p1k4dIrBAaKi","outputId":"e1e0dec5-d059-49c5-c7ae-5c37306d871a","execution":{"iopub.status.busy":"2022-06-01T02:32:56.799517Z","iopub.execute_input":"2022-06-01T02:32:56.799999Z","iopub.status.idle":"2022-06-01T02:32:56.811860Z","shell.execute_reply.started":"2022-06-01T02:32:56.799977Z","shell.execute_reply":"2022-06-01T02:32:56.811324Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654051950026,"user_tz":-480,"elapsed":50,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Discrete(4)\n"]}]},{"cell_type":"markdown","source":["`Discrete(4)` implies that there are four kinds of actions can be taken by agent.\n","- 0 implies the agent will not take any actions\n","- 2 implies the agent will accelerate downward\n","- 1, 3 implies the agent will accelerate left and right\n","\n","Next, we will try to make the agent interact with the environment. \n","Before taking any actions, we recommend to call `reset()` function to reset the environment. Also, this function will return the initial state of the environment."],"metadata":{"id":"dejXT6PHBrPn"}},{"cell_type":"code","source":["initial_state = env.reset()\n","print(initial_state)"],"metadata":{"id":"pi4OmrmZgnWA","outputId":"c428d461-47bb-4ada-bd1e-1ca33e1ff545","execution":{"iopub.status.busy":"2022-06-01T02:32:56.812793Z","iopub.execute_input":"2022-06-01T02:32:56.813078Z","iopub.status.idle":"2022-06-01T02:32:56.828000Z","shell.execute_reply.started":"2022-06-01T02:32:56.813055Z","shell.execute_reply":"2022-06-01T02:32:56.827549Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654051950487,"user_tz":-480,"elapsed":504,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.00396109  1.4083536   0.40119505 -0.11407257 -0.00458307 -0.09087662\n","  0.          0.        ]\n"]}]},{"cell_type":"markdown","source":["Then, we try to get a random action from the agent's action space."],"metadata":{"id":"uBx0mEqqgxJ9"}},{"cell_type":"code","source":["random_action = env.action_space.sample()\n","print(random_action)"],"metadata":{"id":"vxkOEXRKgizt","outputId":"7a549440-01c7-4ecb-b3c6-97b3b0c96e6d","execution":{"iopub.status.busy":"2022-06-01T02:32:56.829217Z","iopub.execute_input":"2022-06-01T02:32:56.829414Z","iopub.status.idle":"2022-06-01T02:32:56.841394Z","shell.execute_reply.started":"2022-06-01T02:32:56.829387Z","shell.execute_reply":"2022-06-01T02:32:56.840784Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654051950489,"user_tz":-480,"elapsed":57,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"markdown","source":["More, we can utilize `step()` to make agent act according to the randomly-selected `random_action`.\n","The `step()` function will return four values:\n","- observation / state\n","- reward\n","- done (True/ False)\n","- Other information"],"metadata":{"id":"mns-bO01g0-J"}},{"cell_type":"code","source":["observation, reward, done, info = env.step(random_action)"],"metadata":{"id":"E_WViSxGgIk9","execution":{"iopub.status.busy":"2022-06-01T02:32:56.842642Z","iopub.execute_input":"2022-06-01T02:32:56.843020Z","iopub.status.idle":"2022-06-01T02:32:56.858877Z","shell.execute_reply.started":"2022-06-01T02:32:56.842986Z","shell.execute_reply":"2022-06-01T02:32:56.858079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(done)"],"metadata":{"id":"yK7r126kuCNp","outputId":"69513e5b-0c65-43f4-84b7-2ae401bbd8e0","execution":{"iopub.status.busy":"2022-06-01T02:32:56.860164Z","iopub.execute_input":"2022-06-01T02:32:56.860608Z","iopub.status.idle":"2022-06-01T02:32:56.886643Z","shell.execute_reply.started":"2022-06-01T02:32:56.860568Z","shell.execute_reply":"2022-06-01T02:32:56.885318Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654051950493,"user_tz":-480,"elapsed":55,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["### Reward\n","\n","\n","> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. "],"metadata":{"id":"GKdS8vOihxhc"}},{"cell_type":"code","source":["print(reward)"],"metadata":{"id":"vxQNs77hi0_7","outputId":"f4e6638e-5611-4034-829f-1b86167c28bc","execution":{"iopub.status.busy":"2022-06-01T02:32:56.887822Z","iopub.execute_input":"2022-06-01T02:32:56.888365Z","iopub.status.idle":"2022-06-01T02:32:56.918383Z","shell.execute_reply.started":"2022-06-01T02:32:56.888340Z","shell.execute_reply":"2022-06-01T02:32:56.917658Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654051950495,"user_tz":-480,"elapsed":54,"user":{"displayName":"孫志傑 Sun Chih Chieh E24932223","userId":"15814509083696715096"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-0.8588900517154912\n"]}]},{"cell_type":"markdown","source":["### Random Agent\n","In the end, before we start training, we can see whether a random agent can successfully land the moon or not."],"metadata":{"id":"Mhqp6D-XgHpe"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()\n","\n","env.reset()\n","\n","img = plt.imshow(env.render(mode='rgb_array'))\n","\n","done = False\n","rewards = []\n","while not done:\n","    action = env.action_space.sample()\n","    observation, reward, done, _ = env.step(action)\n","    \n","    rewards.append(reward)\n","    img.set_data(env.render(mode='rgb_array'))\n","    display.display(plt.gcf())\n","    display.clear_output(wait=True)\n","print(np.mean(rewards))\n","virtual_display.stop()"],"metadata":{"id":"Y3G0bxoccelv","execution":{"iopub.status.busy":"2022-06-01T02:32:56.919372Z","iopub.execute_input":"2022-06-01T02:32:56.919788Z","iopub.status.idle":"2022-06-01T02:32:56.946272Z","shell.execute_reply.started":"2022-06-01T02:32:56.919762Z","shell.execute_reply":"2022-06-01T02:32:56.945628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"nTueL8QbJD3B"}},{"cell_type":"code","source":["class Actor(nn.Module):\n","\n","    def __init__(self, in_dim, out_dim):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(in_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, out_dim),\n","            nn.Softmax(dim=-1),\n","        )\n","\n","    def forward(self, state):\n","        return self.model(state)"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:56.957904Z","iopub.execute_input":"2022-06-01T02:32:56.958359Z","iopub.status.idle":"2022-06-01T02:32:56.979805Z","shell.execute_reply.started":"2022-06-01T02:32:56.958319Z","shell.execute_reply":"2022-06-01T02:32:56.978380Z"},"trusted":true,"id":"lcuYXvQOJD3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Critic(nn.Module):\n","\n","    def __init__(self, in_dim, out_dim):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(in_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, out_dim),\n","        )\n","\n","    def forward(self, state):\n","        return self.model(state)"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:57.147581Z","iopub.execute_input":"2022-06-01T02:32:57.147854Z","iopub.status.idle":"2022-06-01T02:32:57.154793Z","shell.execute_reply.started":"2022-06-01T02:32:57.147820Z","shell.execute_reply":"2022-06-01T02:32:57.153847Z"},"trusted":true,"id":"MDwUJyXrJD3D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## utils"],"metadata":{"id":"yGknG5csJD3D"}},{"cell_type":"markdown","source":["### Test Env \n","作弊用 only for 作業，現實不可能"],"metadata":{"id":"jQUbWcWAJD3A"}},{"cell_type":"code","source":["def test_agent(agent, env, seed=543):\n","    fix(env, seed)\n","    agent.eval()  # set the network into evaluation mode\n","    NUM_OF_TEST = 5 # Do not revise this !!!\n","    test_total_reward = []\n","    for i in range(NUM_OF_TEST):\n","        state = env.reset()\n","\n","        total_reward = 0\n","\n","        done = False\n","        while not done:\n","            action, _ = agent.action(np.expand_dims(state, axis=0))\n","            state, reward, done, _ = env.step(action.squeeze())\n","\n","            total_reward += reward\n","\n","        test_total_reward.append(total_reward)\n","    \n","    agent.train()\n","    return np.mean(test_total_reward)"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:56.947722Z","iopub.execute_input":"2022-06-01T02:32:56.948538Z","iopub.status.idle":"2022-06-01T02:32:56.956704Z","shell.execute_reply.started":"2022-06-01T02:32:56.948494Z","shell.execute_reply":"2022-06-01T02:32:56.956070Z"},"trusted":true,"id":"aYCpQ2qUJD3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Play one and collect data"],"metadata":{"id":"vRGtz56oJW8M"}},{"cell_type":"code","source":["def play_one_episode(env, agent, gamma, max_ep_len):\n","    state = env.reset()\n","    \n","    states = []\n","    actions = []\n","    log_probs = []\n","    rewards = []\n","    rewards_acc = []\n","    next_states = []\n","    dones = []\n","\n","    for _ in range(max_ep_len):\n","        # take action\n","        action, log_prob = agent.action(np.expand_dims(state,axis=0))\n","\n","        # interact\n","        next_state, reward, done, _ = env.step(action.squeeze())\n","\n","        # store trajectory\n","        states.append(state)\n","        actions.append(action)\n","        log_probs.append([log_prob]) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n","        rewards.append([reward])\n","        next_states.append(next_state)\n","        dones.append([done])\n","        \n","        state = next_state\n","        if done:\n","            break\n","\n","    # ! IMPORTANT !\n","    # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n","    #                                                         rewards :     r1, r2 ,r3 ......\n","    # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n","    #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n","    # boss : implement Actor-Critic\n","    R = 0\n","    for r in rewards[::-1]:\n","        R = r[0] + gamma * R\n","        rewards_acc.insert(0, [R])\n","    \n","    # same batch size\n","    batch = len(states)\n","    assert np.shape(states) == (batch, 8)\n","    assert np.shape(actions) == (batch, 1)\n","    assert torch.Tensor(log_probs).shape == (batch, 1)\n","    assert np.shape(rewards) == (batch, 1)\n","    assert np.shape(rewards_acc) == (batch, 1)\n","    assert np.shape(next_states) == (batch, 8)\n","    assert np.shape(dones) == (batch, 1)\n","    \n","    return states, actions, log_probs, rewards, rewards_acc, next_states, dones"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:57.045666Z","iopub.execute_input":"2022-06-01T02:32:57.046101Z","iopub.status.idle":"2022-06-01T02:32:57.055266Z","shell.execute_reply.started":"2022-06-01T02:32:57.046079Z","shell.execute_reply":"2022-06-01T02:32:57.054629Z"},"trusted":true,"id":"-OwZSa0bJD3E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Policy Gradient\n","Now, we can build a simple policy network. The network will return one of action in the action space."],"metadata":{"id":"F5paWqo7tWL2"}},{"cell_type":"code","source":["class PolicyGradientAgent(nn.Module):\n","    def __init__(self, actor, optimizer):\n","        super().__init__()\n","        self.actor = actor\n","        self.optimizer = optimizer\n","        \n","    def forward(self, x):\n","        return self.actor(x)\n","    \n","    def action(self, state):\n","        action_prob = self(torch.FloatTensor(state))\n","        action_dist = Categorical(action_prob)\n","        action = action_dist.sample()\n","        log_prob = action_dist.log_prob(action)\n","        \n","        return action.cpu().numpy(), log_prob\n","    \n","    def learn(self, log_probs, rewards):\n","        # use torch.stack to remain gradient\n","        log_probs = torch.stack([log_prob[0] for log_prob in log_probs])  # (batch, 1)\n","        rewards = FloatTensor(rewards)                                    # (batch, 1)\n","\n","        # train\n","        rewards_norm = (rewards - rewards.mean()) / (rewards.std() + 1e-9)  # !important: normalize the reward to make negative & postive reward\n","        loss = -log_probs * rewards_norm\n","        loss = loss.sum() # must be sum\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        \n","        return loss.item()"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:56.981004Z","iopub.execute_input":"2022-06-01T02:32:56.981251Z","iopub.status.idle":"2022-06-01T02:32:56.993713Z","shell.execute_reply.started":"2022-06-01T02:32:56.981216Z","shell.execute_reply":"2022-06-01T02:32:56.993056Z"},"trusted":true,"id":"LOy_olgoJD3G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lastly, build a network and agent to start training."],"metadata":{"id":"ehPlnTKyRZf9"}},{"cell_type":"code","source":["epochs = 2000\n","episodes = 5\n","max_ep_len = 300\n","gamma = 0.99\n","\n","learning_rate = 0.001"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:56.998072Z","iopub.execute_input":"2022-06-01T02:32:56.999073Z","iopub.status.idle":"2022-06-01T02:32:57.013247Z","shell.execute_reply.started":"2022-06-01T02:32:56.999006Z","shell.execute_reply":"2022-06-01T02:32:57.012610Z"},"trusted":true,"id":"iTJea69YJD3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actor = Actor(8, 4)\n","optimizer = optim.SGD(actor.parameters(), lr=learning_rate) # !important SGD\n","agent_policygradient = PolicyGradientAgent(actor, optimizer)\n","# agent_policygradient.load_state_dict(torch.load(\"../input/hw12tmp/agent_policygradient.pt\"))"],"metadata":{"id":"GfJIvML-RYjL","execution":{"iopub.status.busy":"2022-06-01T02:32:57.014324Z","iopub.execute_input":"2022-06-01T02:32:57.015353Z","iopub.status.idle":"2022-06-01T02:32:57.044510Z","shell.execute_reply.started":"2022-06-01T02:32:57.015315Z","shell.execute_reply":"2022-06-01T02:32:57.043696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training Agent\n","\n","Now let's start to train our agent.\n","Through taking all the interactions between agent and environment as training data, the policy network can learn from all these attempts,"],"metadata":{"id":"ouv23glgf5Qt"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","agent_policygradient.train()\n","best_avg_total_reward = -np.inf\n","best_test_total_reward = -np.inf\n","avg_total_rewards, avg_final_rewards = [], []\n","total_rewards, final_rewards, total_loss = deque(maxlen=100), deque(maxlen=100), deque(maxlen=100)\n","progress_bar = tqdm(range(epochs))\n","for epoch in progress_bar:\n","    log_probs_epoch, rewards_epoch = [], []\n","    for episode in range(episodes):  # Don't infinite loop while learning\n","        states, actions, log_probs, rewards, rewards_acc, next_states, dones = play_one_episode(env, agent_policygradient, gamma, max_ep_len)\n","        \n","        log_probs_epoch.extend(log_probs)\n","        rewards_epoch.extend(rewards_acc)\n","        \n","        total_rewards.append(np.sum(rewards))\n","        final_rewards.append(rewards[-1])\n","    \n","    loss = agent_policygradient.learn(log_probs_epoch, rewards_epoch)\n","    total_loss.append(loss)\n","    \n","    avg_total_reward = np.mean(total_rewards)\n","    avg_final_reward = np.mean(final_rewards)\n","    avg_total_rewards.append(avg_total_reward)\n","    avg_final_rewards.append(avg_final_reward)\n","    \n","    if best_avg_total_reward < avg_total_reward:\n","#         torch.save(agent_policygradient.state_dict(), \"agent_policygradient.pt\")\n","        best_avg_total_reward = avg_total_reward\n","    \n","    test_total_reward = test_agent(agent_policygradient, env)\n","    if best_test_total_reward < test_total_reward:\n","        torch.save(agent_policygradient.state_dict(), \"agent_policygradient.pt\")\n","        best_test_total_reward = test_total_reward\n","        print(f\"{epoch:4d}: best test {best_test_total_reward:.4f}, avg:{avg_total_reward:.4f}, best avg:{best_avg_total_reward:.4f}\")\n","    \n","    progress_bar.set_postfix(Total=avg_total_reward, \n","                             Total_best=best_avg_total_reward,\n","                             Test=test_total_reward,\n","                             Test_best=best_test_total_reward,\n","                             Final=avg_final_reward, \n","                             loss=np.mean(total_loss))"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:57.056818Z","iopub.execute_input":"2022-06-01T02:32:57.057036Z","iopub.status.idle":"2022-06-01T02:32:57.089926Z","shell.execute_reply.started":"2022-06-01T02:32:57.057007Z","shell.execute_reply":"2022-06-01T02:32:57.089141Z"},"trusted":true,"id":"a43WPh8yJD3N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training Result\n","During the training process, we recorded `avg_total_reward`, which represents the average total reward of episodes before updating the policy network.\n","\n","Theoretically, if the agent becomes better, the `avg_total_reward` will increase.\n","The visualization of the training process is shown below:  \n","\n","In addition, `avg_final_reward` represents average final rewards of episodes. To be specific, final rewards is the last reward received in one episode, indicating whether the craft lands successfully or not."],"metadata":{"id":"vNb_tuFYhKVK"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","plt.plot(avg_total_rewards)\n","plt.title(\"Total Rewards\")\n","plt.show()\n","\n","plt.plot(avg_final_rewards)\n","plt.title(\"Final Rewards\")\n","plt.show()"],"metadata":{"id":"wZYOI8H10SHN","execution":{"iopub.status.busy":"2022-06-01T02:32:57.092005Z","iopub.execute_input":"2022-06-01T02:32:57.092370Z","iopub.status.idle":"2022-06-01T02:32:57.109786Z","shell.execute_reply.started":"2022-06-01T02:32:57.092338Z","shell.execute_reply":"2022-06-01T02:32:57.109166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PPO(Proximal Policy Optimization)\n","* https://huggingface.co/ThomasSimonini/ppo-LunarLander-v2\n","* https://github.com/nikhilbarhate99/PPO-PyTorch\n","* https://hackmd.io/@shaoeChen/Bywb8YLKS/https%3A%2F%2Fhackmd.io%2F%40shaoeChen%2FSyez2AmFr"],"metadata":{"id":"HWg9XCGbJD3Q"}},{"cell_type":"markdown","source":["### ref model to imporve that it could work"],"metadata":{"id":"mXyz8pq9JD3R"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","!pip install stable-baselines3"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:57.111198Z","iopub.execute_input":"2022-06-01T02:32:57.111565Z","iopub.status.idle":"2022-06-01T02:32:57.127161Z","shell.execute_reply.started":"2022-06-01T02:32:57.111526Z","shell.execute_reply":"2022-06-01T02:32:57.126390Z"},"trusted":true,"id":"KZwYhLZDJD3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","from stable_baselines3 import PPO\n","\n","\n","model = PPO('MlpPolicy', env, verbose=1)\n","model.learn(total_timesteps=100e3)\n","        \n","obs = env.reset()\n","img = plt.imshow(env.render(mode='rgb_array'))\n","\n","done = False\n","rewards = []\n","while not done:\n","    action, _states = model.predict(obs)\n","    obs, reward, done, _ = env.step(action)\n","    \n","    rewards.append(reward)\n","    img.set_data(env.render(mode='rgb_array'))\n","    display.display(plt.gcf())\n","    display.clear_output(wait=True)\n","print(np.mean(rewards))"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:32:57.128395Z","iopub.execute_input":"2022-06-01T02:32:57.128774Z","iopub.status.idle":"2022-06-01T02:32:57.145952Z","shell.execute_reply.started":"2022-06-01T02:32:57.128750Z","shell.execute_reply":"2022-06-01T02:32:57.144804Z"},"trusted":true,"id":"7IEzjT0cJD3S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### implement"],"metadata":{"id":"--yXbCe4JD3T"}},{"cell_type":"code","source":["class PPOAgent(nn.Module):\n","    def __init__(self, actor, critic, optimizer_actor, optimizer_critic, gamma, train_epochs=5, clip=0.2):\n","        super().__init__()\n","        self.actor = actor\n","        self.critic = critic\n","        self.optimizer_actor = optimizer_actor\n","        self.optimizer_critic = optimizer_critic\n","        self.gamma = gamma\n","        self.train_epochs = train_epochs\n","        self.clip = clip\n","        \n","    def forward(self, x):\n","        return self.actor(x)\n","    \n","    def action(self, state):\n","        action_prob = self(torch.FloatTensor(state))\n","        action_dist = Categorical(action_prob)\n","        action = action_dist.sample()\n","        log_prob = action_dist.log_prob(action)\n","        \n","        return action.cpu().numpy(), log_prob\n","    \n","    def evaluate_action(self, state, action):\n","        action_prob = self(state)\n","        action_dist = Categorical(action_prob)\n","\n","        log_prob = action_dist.log_prob(action.squeeze()).unsqueeze(1)\n","        entropy = action_dist.entropy().unsqueeze(1)\n","        \n","        return log_prob, entropy\n","    \n","    def learn(self, states, actions, next_states, log_probs, rewards, rewards_acc):\n","        # use FloatTensor to replce torch.from_numpy to avoid RuntimeError: Found dtype Double but expected Float\n","        states = FloatTensor(states)                      # (batch, 8)\n","        actions = torch.LongTensor(actions)               # (batch, 1)\n","        next_states = FloatTensor(next_states)            # (batch, 8)\n","        old_log_probs = FloatTensor(log_probs).detach()   # (batch, 1)\n","        rewards = FloatTensor(rewards)                    # (batch, 1)\n","        rewards_acc = FloatTensor(rewards_acc)            # (batch, 1)        \n","        \n","        loss_actors = []\n","        loss_critics = []\n","        loss_entropys = []\n","        for _ in range(self.train_epochs): \n","            # critic ========================================================\n","            V = self.critic(states)                       # (batch, 1)\n","#             V_next = self.critic(next_states)             # (batch, 1)\n","            # A = r + gamma * V_s_t+1 - V_s_t\n","#             A = rewards + self.gamma * V_next.detach() - V.detach() \n","            A = rewards_acc - V\n","        \n","            loss_critic = A.pow(2).mean()\n","            self.optimizer_critic.zero_grad()\n","            loss_critic.backward()\n","            self.optimizer_critic.step()\n","            \n","            loss_critics.append(loss_critic.item())\n","        \n","        V = self.critic(states)\n","        A = rewards_acc - V\n","        for _ in range(self.train_epochs): \n","            # actor ========================================================      \n","            new_log_probs, entropies = self.evaluate_action(states, actions) # same action to calculate right ratios\n","            # (batch, 1)   (batch, 1)\n","            \n","            # important sampling\n","            # Calculate the ratio pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)\n","            ratios = torch.exp(new_log_probs - old_log_probs)                            # (batch, 1)\n","            \n","            # losses.\n","            loss1 = ratios * A.detach()                                                  # (batch, 1)\n","            loss_limit = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A.detach()  # (batch, 1)\n","            # larger entropies more exploration\n","            loss_actor = (-torch.min(loss1, loss_limit)) - 0.01 * entropies              # (batch, 1)\n","            loss_actor = loss_actor.mean()\n","            self.optimizer_actor.zero_grad()\n","            loss_actor.backward()\n","            self.optimizer_actor.step()\n","            \n","            loss_actors.append(loss_actor.item())\n","            loss_entropys.append(entropies.mean().item())\n","        \n","        return loss_actors, loss_critics, loss_entropys"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:33:32.105153Z","iopub.execute_input":"2022-06-01T02:33:32.105551Z","iopub.status.idle":"2022-06-01T02:33:32.120590Z","shell.execute_reply.started":"2022-06-01T02:33:32.105528Z","shell.execute_reply":"2022-06-01T02:33:32.119590Z"},"trusted":true,"id":"8Im6nIENJD3U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 2000\n","episodes = 3\n","max_ep_len = 300\n","gamma = 0.99"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:33:33.284392Z","iopub.execute_input":"2022-06-01T02:33:33.284713Z","iopub.status.idle":"2022-06-01T02:33:33.288560Z","shell.execute_reply.started":"2022-06-01T02:33:33.284686Z","shell.execute_reply":"2022-06-01T02:33:33.287533Z"},"trusted":true,"id":"JwEX70uaJD3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actor = Actor(8, 4)\n","critic = Critic(8, 1)\n","optimizer_actor = optim.Adam(actor.parameters(), lr=0.0003)\n","optimizer_critic = optim.Adam(critic.parameters(), lr=0.001)\n","agent_PPO = PPOAgent(actor, critic, optimizer_actor, optimizer_critic, gamma, train_epochs=30)\n","# agent_PPO.load_state_dict(torch.load(\"../input/hw12tmp/agent_PPO.pt\"))"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:33:34.127815Z","iopub.execute_input":"2022-06-01T02:33:34.128399Z","iopub.status.idle":"2022-06-01T02:33:34.134468Z","shell.execute_reply.started":"2022-06-01T02:33:34.128372Z","shell.execute_reply":"2022-06-01T02:33:34.133720Z"},"trusted":true,"id":"lMtpYB9nJD3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","agent_PPO.train()\n","best_avg_total_reward = -np.inf\n","best_test_total_reward = -np.inf\n","avg_total_rewards, avg_final_rewards = [], []\n","total_rewards, final_rewards = deque(maxlen=100), deque(maxlen=100)\n","total_loss_actors, total_loss_critics, total_loss_entropys = deque(maxlen=100), deque(maxlen=100), deque(maxlen=100)\n","progress_bar = tqdm(range(epochs))\n","for epoch in progress_bar:\n","    states_epoch, actions_epoch, log_probs_epoch, rewards_epoch, rewards_acc_epoch, next_states_epoch = [], [], [], [], [], []\n","    for episode in range(episodes):  # Don't infinite loop while learning\n","        states, actions, log_probs, rewards, rewards_acc, next_states, dones = play_one_episode(env, agent_PPO, gamma, max_ep_len)\n","        \n","        states_epoch.extend(states)\n","        actions_epoch.extend(actions)\n","        log_probs_epoch.extend(log_probs)\n","        rewards_epoch.extend(rewards)\n","        rewards_acc_epoch.extend(rewards_acc)\n","        next_states_epoch.extend(next_states)\n","        \n","        total_rewards.append(np.sum(rewards))\n","        final_rewards.append(rewards[-1])\n","    \n","    loss_actors, loss_critics, loss_entropys = agent_PPO.learn(states_epoch, actions_epoch, next_states_epoch, log_probs_epoch, rewards_epoch, rewards_acc_epoch)\n","    \n","    avg_total_reward = np.mean(total_rewards)\n","    avg_final_reward = np.mean(final_rewards)\n","    avg_total_rewards.append(avg_total_reward)\n","    avg_final_rewards.append(avg_final_reward)\n","    total_loss_actors.extend(loss_actors)\n","    total_loss_critics.extend(loss_critics)\n","    total_loss_entropys.extend(loss_entropys)\n","    \n","    if best_avg_total_reward < avg_total_reward:\n","#         torch.save(agent_PPO.state_dict(), \"agent_PPO.pt\")\n","        best_avg_total_reward = avg_total_reward\n","      \n","    test_total_reward = test_agent(agent_PPO, env)\n","    if best_test_total_reward < test_total_reward:\n","        torch.save(agent_PPO.state_dict(), \"agent_PPO.pt\")\n","        best_test_total_reward = test_total_reward\n","        print(f\"{epoch:4d}: best test {best_test_total_reward:.4f}, avg:{avg_total_reward:.4f}, best avg:{best_avg_total_reward:.4f}\")\n","    \n","    progress_bar.set_postfix(Total=avg_total_reward, \n","                             Total_best=best_avg_total_reward,\n","                             Test=test_total_reward,\n","                             Test_best=best_test_total_reward,\n","                             Final=avg_final_reward, \n","                             loss_actors=np.mean(total_loss_actors),\n","                             loss_critics=np.mean(total_loss_critics),\n","                             loss_entropys=np.mean(total_loss_entropys))"],"metadata":{"execution":{"iopub.status.busy":"2022-06-01T02:33:35.470501Z","iopub.execute_input":"2022-06-01T02:33:35.470797Z"},"trusted":true,"id":"ibCfydidJD3Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","plt.plot(avg_total_rewards)\n","plt.title(\"Total Rewards\")\n","plt.show()\n","\n","plt.plot(avg_final_rewards)\n","plt.title(\"Final Rewards\")\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:31:25.232264Z","iopub.execute_input":"2022-05-30T06:31:25.232682Z","iopub.status.idle":"2022-05-30T06:31:25.544617Z","shell.execute_reply.started":"2022-05-30T06:31:25.232642Z","shell.execute_reply":"2022-05-30T06:31:25.543723Z"},"trusted":true,"id":"GZirzDOhJD3c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DQN\n","* https://github.com/higgsfield/RL-Adventure\n","* https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/07/DQN-LunarLander.html\n","* https://github.com/Curt-Park/rainbow-is-all-you-need"],"metadata":{"id":"WO4PJo-CJD3d"}},{"cell_type":"markdown","source":["### Replay Buffer"],"metadata":{"id":"ipO5IO7kJD3e"}},{"cell_type":"code","source":["class ReplayBuffer(object):\n","    def __init__(self, capacity):\n","        self.buffer = deque(maxlen=capacity)\n","    \n","    def push(self, state, action, reward, next_state, done):            \n","        self.buffer.append((state, action, [reward], next_state, [done]))\n","    \n","    def sample(self, batch_size):\n","        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n","        assert np.shape(state) == (batch_size, 8)\n","        assert np.shape(action) == (batch_size, 1)\n","        assert np.shape(reward) == (batch_size, 1)\n","        assert np.shape(next_state) == (batch_size, 8)\n","        assert np.shape(done) == (batch_size, 1)\n","        \n","        return state, action, reward, next_state, done\n","    \n","    def __len__(self):\n","        return len(self.buffer)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:25:23.299743Z","iopub.execute_input":"2022-05-31T13:25:23.300532Z","iopub.status.idle":"2022-05-31T13:25:23.314413Z","shell.execute_reply.started":"2022-05-31T13:25:23.300496Z","shell.execute_reply":"2022-05-31T13:25:23.313803Z"},"trusted":true,"id":"paEFmC5nJD3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NoisyLinear(nn.Module):\n","    def __init__(self, in_features, out_features, std_init=0.4):\n","        super(NoisyLinear, self).__init__()\n","        \n","        self.in_features  = in_features\n","        self.out_features = out_features\n","        self.std_init     = std_init\n","        \n","        self.weight_mu    = nn.Parameter(torch.FloatTensor(out_features, in_features))\n","        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n","        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n","        \n","        self.bias_mu    = nn.Parameter(torch.FloatTensor(out_features))\n","        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n","        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n","        \n","        self.reset_parameters()\n","        self.reset_noise()\n","    \n","    def forward(self, x):\n","        if self.training: \n","            weight = self.weight_mu + self.weight_sigma  * self.weight_epsilon\n","            bias   = self.bias_mu   + self.bias_sigma * self.bias_epsilon\n","        else:\n","            weight = self.weight_mu\n","            bias   = self.bias_mu\n","        \n","        return F.linear(x, weight, bias)\n","    \n","    def reset_parameters(self):\n","        mu_range = 1 / math.sqrt(self.weight_mu.size(1))\n","        \n","        self.weight_mu.data.uniform_(-mu_range, mu_range)\n","        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.weight_sigma.size(1))) # 1/sqrt(in_features)\n","        \n","        self.bias_mu.data.uniform_(-mu_range, mu_range)\n","        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.bias_sigma.size(0)))     # 1/sqrt(out_features)\n","    \n","    def reset_noise(self):\n","        epsilon_in  = self._scale_noise(self.in_features)\n","        epsilon_out = self._scale_noise(self.out_features)\n","        \n","        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n","        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n","    \n","    def _scale_noise(self, size):\n","        x = torch.randn(size)\n","        x = x.sign().mul(x.abs().sqrt())\n","        return x"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:25:23.315552Z","iopub.execute_input":"2022-05-31T13:25:23.31596Z","iopub.status.idle":"2022-05-31T13:25:23.327928Z","shell.execute_reply.started":"2022-05-31T13:25:23.315931Z","shell.execute_reply":"2022-05-31T13:25:23.327353Z"},"trusted":true,"id":"PcJngb8_JD3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class QNet(nn.Module):\n","\n","    def __init__(self, in_dim, out_dim):\n","        super().__init__()\n","        # Dueling-DQN\n","        self.net = nn.Sequential(\n","            nn.Linear(in_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","            nn.ReLU(),\n","        )\n","        self.adv = nn.Sequential(\n","            nn.Linear(64, 1),\n","        )\n","        self.value = nn.Sequential(\n","            nn.Linear(64, out_dim),\n","        )        \n","\n","    def forward(self, state):\n","        x = self.net(state)\n","        adv = self.adv(x)\n","        value = self.value(x)\n","        value = value - value.mean(dim=1).unsqueeze(1)\n","        return value + adv\n","    \n","class QNetNoise(nn.Module):\n","\n","    def __init__(self, in_dim, out_dim):\n","        super().__init__()        \n","        # NoisyNet & Dueling-DQN\n","        self.linear = nn.Linear(in_dim, 64)\n","        self.noise_layer = NoisyLinear(64, 64)\n","        self.noise_layer_adv = NoisyLinear(64, 1)\n","        self.noise_layer_value = NoisyLinear(64, out_dim)\n","        \n","\n","    def forward(self, state):\n","        x = F.relu(self.linear(state))\n","        x = F.relu(self.noise_layer(x))\n","        \n","        adv = self.noise_layer_adv(x)\n","        value = self.noise_layer_value(x)\n","        value = value - value.mean(dim=1).unsqueeze(1)\n","        return value + adv\n","    \n","    def reset_noise(self):\n","        self.noise_layer.reset_noise()\n","        self.noise_layer_adv.reset_noise()\n","        self.noise_layer_value.reset_noise()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:25:23.329106Z","iopub.execute_input":"2022-05-31T13:25:23.329436Z","iopub.status.idle":"2022-05-31T13:25:23.345985Z","shell.execute_reply.started":"2022-05-31T13:25:23.329405Z","shell.execute_reply":"2022-05-31T13:25:23.344883Z"},"trusted":true,"id":"j0xDhkefJD3g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DQNAgent(nn.Module):\n","    def __init__(self, qNet_eval, qNet_target, optimizer_qNet, gamma, tau, epsilon=0.01, train_epochs=5):\n","        super().__init__()\n","        self.qNet_eval = qNet_eval\n","        self.optimizer_qNet_eval = optimizer_qNet\n","        self.qNet_target = qNet_target\n","        self.qNet_target.load_state_dict(self.qNet_eval.state_dict())\n","        self.gamma = gamma\n","        self.tau = tau\n","        self.epsilon = epsilon\n","        self.train_epochs = train_epochs\n","        \n","    def forward(self, x):\n","        return self.qNet_eval(x)\n","    \n","    def action(self, state):\n","#         # Epsilon-greedy action selection\n","#         # use simple epsilon to do better work\n","#         if random.random() > self.epsilon:\n","#             action_prob = self.qNet_eval(torch.FloatTensor(state))\n","#             log_prob, action = torch.max(action_prob, dim=1)\n","\n","#             return action.cpu().numpy(), log_prob\n","#         else:\n","#             return np.array([random.choice(np.arange(4))]), None\n","\n","        # NoisyNet: no epsilon greedy action selection\n","        action_prob = self.qNet_eval(torch.FloatTensor(state))\n","        log_prob, action = torch.max(action_prob, dim=1)\n","\n","        return action.cpu().numpy(), log_prob\n","    \n","    def learn(self, replay_buffer):\n","        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n","        \n","        # use FloatTensor to replce torch.from_numpy to avoid RuntimeError: Found dtype Double but expected Float\n","        states      = FloatTensor(state)\n","        next_states = FloatTensor(next_state)\n","        actions     = torch.LongTensor(action)\n","        rewards     = FloatTensor(reward)\n","        dones       = torch.LongTensor(done)\n","        \n","        loss_qNet_evals = []\n","        for _ in range(self.train_epochs): \n","            Q = self.qNet_eval(states)                     # (batch, 4)\n","            Q_next = self.qNet_eval(next_states)           # (batch, 4)\n","            Q_next_target = self.qNet_target(next_states)  # (batch, 4)\n","            \n","            # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n","            #       = r                       otherwise\n","            # Double-DQN\n","            # 在 dim=1，以 a 為 index 取值\n","            # ref:https://zhuanlan.zhihu.com/p/352877584\n","            Q = Q.gather(dim=1, index=actions)                               # (batch, 1)\n","            actions_next = torch.argmax(Q_next, dim=1, keepdim=True)         # (batch, 1)\n","            Q_next_target = Q_next_target.gather(dim=1, index=actions_next)  # (batch, 1)\n","            Q_expect = rewards + self.gamma * Q_next_target * (1 - dones)    # (batch, 1)\n","            \n","#             loss_qNet_eval = F.mse_loss(Q, Q_expect.detach()) # TD Error\n","            loss_qNet_eval = F.smooth_l1_loss(Q, Q_expect.detach()) # TD Error\n","            self.optimizer_qNet_eval.zero_grad()\n","            loss_qNet_eval.backward()\n","            self.optimizer_qNet_eval.step()\n","            \n","            loss_qNet_evals.append(loss_qNet_eval.item())\n","        \n","        self.qNet_eval.reset_noise()\n","        self.qNet_target.reset_noise()\n","#         self.qNet_target.load_state_dict(self.qNet_eval.state_dict())\n","        self.soft_update(qNet_eval, qNet_target, self.tau)\n","        \n","        return loss_qNet_evals\n","    \n","    def soft_update(self, eval_model, target_model, tau):\n","        \"\"\"Soft update model parameters.\n","        θ_target = τ*θ_eval + (1 - τ)*θ_target\n","\n","        Params\n","        ======\n","            eval_param   (PyTorch model): weights will be copied from\n","            target_model (PyTorch model): weights will be copied to\n","            tau (float): interpolation parameter \n","        \"\"\"\n","        for target_param, eval_param in zip(target_model.parameters(), eval_model.parameters()):\n","            target_param.data.copy_(tau*eval_param.data + (1.0-tau)*target_param.data)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:25:23.347735Z","iopub.execute_input":"2022-05-31T13:25:23.348224Z","iopub.status.idle":"2022-05-31T13:25:23.36499Z","shell.execute_reply.started":"2022-05-31T13:25:23.348182Z","shell.execute_reply":"2022-05-31T13:25:23.364143Z"},"trusted":true,"id":"aZqETZOpJD3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 2000\n","episodes = 5\n","max_ep_len = 300\n","gamma = 0.99\n","batch_size = 64\n","TAU = 1e-3              \n","learning_rate = 0.0001 # 5e-4   \n","replay_size = 10000 #1000"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:25:23.366167Z","iopub.execute_input":"2022-05-31T13:25:23.36646Z","iopub.status.idle":"2022-05-31T13:25:23.379769Z","shell.execute_reply.started":"2022-05-31T13:25:23.36642Z","shell.execute_reply":"2022-05-31T13:25:23.378835Z"},"trusted":true,"id":"l0rFe_j5JD3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["replay_buffer = ReplayBuffer(replay_size)\n","\n","qNet_eval = QNetNoise(8, 4)\n","qNet_target = QNetNoise(8, 4)\n","optimizer_qNet = optim.Adam(qNet_eval.parameters(), lr=learning_rate)\n","agent_DQN = DQNAgent(qNet_eval, qNet_target, optimizer_qNet, gamma, TAU, train_epochs=1)\n","# agent_DQN.load_state_dict(torch.load(\"../input/hw12tmp/agent_DQN.pt\"))"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:03:45.823553Z","iopub.execute_input":"2022-05-31T01:03:45.824062Z","iopub.status.idle":"2022-05-31T01:03:45.848217Z","shell.execute_reply.started":"2022-05-31T01:03:45.824022Z","shell.execute_reply":"2022-05-31T01:03:45.846985Z"},"trusted":true,"id":"6Q6C-P6TJD3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","agent_DQN.train()\n","avg_total_rewards, avg_final_rewards = [], []\n","best_avg_total_reward = -np.inf\n","best_test_total_reward = -np.inf\n","total_rewards, final_rewards, total_loss = deque(maxlen=100), deque(maxlen=100), deque(maxlen=100)\n","progress_bar = tqdm(range(epochs))\n","for epoch in progress_bar:\n","    for episode in range(episodes):  # Don't infinite loop while learning\n","        state = env.reset()\n","        total_reward = 0\n","        for i in range(max_ep_len):\n","            # take action\n","            action, log_prob = agent_DQN.action(np.expand_dims(state, axis=0))\n","            # interact\n","            next_state, reward, done, _ = env.step(action.squeeze())\n","\n","            replay_buffer.push(state, action, reward, next_state, done)\n","            \n","            # !important training in episode helps train better, if outer loss would not converge\n","            if i%3 == 0 and len(replay_buffer) > batch_size:\n","                loss = agent_DQN.learn(replay_buffer)\n","                total_loss.extend(loss)\n","\n","            state = next_state\n","            if done:\n","                break\n","            \n","            total_reward += reward\n","\n","        total_rewards.append(total_reward)\n","        final_rewards.append(reward)\n","        \n","    avg_total_reward = np.mean(total_rewards)\n","    avg_final_reward = np.mean(final_rewards)\n","    avg_total_rewards.append(avg_total_reward)\n","    avg_final_rewards.append(avg_final_reward)\n","    \n","    if best_avg_total_reward < avg_total_reward:\n","#         torch.save(agent_DQN.state_dict(), \"agent_DQN.pt\")\n","        best_avg_total_reward = avg_total_reward\n","    \n","    test_total_reward = test_agent(agent_DQN, env)\n","    if best_test_total_reward < test_total_reward:\n","        torch.save(agent_DQN.state_dict(), \"agent_DQN.pt\")\n","        best_test_total_reward = test_total_reward\n","        print(f\"{epoch:4d}: best test {best_test_total_reward:.4f}, avg:{avg_total_reward:.4f}, best avg:{best_avg_total_reward:.4f}\")\n","    \n","    progress_bar.set_postfix(Total=avg_total_reward, \n","                             Total_best=best_avg_total_reward,\n","                             Test=test_total_reward,\n","                             Test_best=best_test_total_reward,\n","                             Final=avg_final_reward, \n","                             loss=np.mean(total_loss))"],"metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:32:09.590418Z","iopub.execute_input":"2022-05-30T06:32:09.590747Z","iopub.status.idle":"2022-05-30T06:33:13.681599Z","shell.execute_reply.started":"2022-05-30T06:32:09.59071Z","shell.execute_reply":"2022-05-30T06:33:13.680182Z"},"trusted":true,"id":"nuepA1wXJD3l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","plt.plot(avg_total_rewards)\n","plt.title(\"Total Rewards\")\n","plt.show()\n","\n","plt.plot(avg_final_rewards)\n","plt.title(\"Final Rewards\")\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-30T06:33:40.651608Z","iopub.execute_input":"2022-05-30T06:33:40.652191Z","iopub.status.idle":"2022-05-30T06:33:41.06085Z","shell.execute_reply.started":"2022-05-30T06:33:40.652153Z","shell.execute_reply":"2022-05-30T06:33:41.059462Z"},"trusted":true,"id":"gORXzr5zJD3m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Advantage Actor-Critic"],"metadata":{"id":"7eg8cwXHJD3n"}},{"cell_type":"code","source":["class A2CAgent(nn.Module):\n","    def __init__(self, actor, critic, optimizer_actor, optimizer_critic, gamma, train_actor_epochs, train_critic_epochs):\n","        super().__init__()\n","        self.actor = actor\n","        self.critic = critic\n","        self.optimizer_actor = optimizer_actor\n","        self.optimizer_critic = optimizer_critic\n","        self.gamma = gamma\n","        self.train_actor_epochs = train_actor_epochs\n","        self.train_critic_epochs = train_critic_epochs\n","    \n","    def action(self, state):\n","        action_prob = self.actor(torch.FloatTensor(state))\n","        action_dist = Categorical(action_prob)\n","        action = action_dist.sample()\n","        log_prob = action_dist.log_prob(action)\n","        \n","        return action.cpu().numpy(), log_prob\n","    \n","    def evaluate_action(self, state, action):\n","        action_prob = self.actor(state)\n","        action_dist = Categorical(action_prob)\n","\n","        log_prob = action_dist.log_prob(action.squeeze()).unsqueeze(1)\n","        entropy = action_dist.entropy().unsqueeze(1)\n","        \n","        return log_prob, entropy\n","    \n","    def learn(self, states, actions, next_states, rewards, rewards_acc, dones):\n","        # use FloatTensor to replce torch.from_numpy to avoid RuntimeError: Found dtype Double but expected Float\n","        states = FloatTensor(states)                                      # (batch, 8)\n","        actions = torch.LongTensor(actions)                               # (batch, 1)\n","        next_states = FloatTensor(next_states)                            # (batch, 8)\n","        rewards = FloatTensor(rewards)                                    # (batch, 1)\n","        rewards_acc = FloatTensor(rewards_acc)                            # (batch, 1)        \n","        dones = torch.LongTensor(dones)                                   # (batch, 1)        \n","        \n","        loss_actors = []\n","        loss_critics = []\n","        loss_entropys = []\n","        \n","        for _ in range(self.train_critic_epochs): \n","            # critic ===============================================================\n","            V = self.critic(states)                  # (batch, 1)\n","#             V_next = self.critic(next_states)        # (batch, 1)\n","    #         A = r + gamma * V_s_t+1 - V_s_t\n","    #         A = rewards + self.gamma * V_next * (1-dones) - V  # TD Error\n","            A = rewards_acc - V                    # MC Error\n","\n","            loss_critic = A.pow(2).mean()\n","            self.optimizer_critic.zero_grad()\n","            loss_critic.backward()\n","            self.optimizer_critic.step()\n","            \n","            loss_critics.append(loss_critic.item())\n","        \n","        V = self.critic(states)                  # (batch, 1)\n","#         V_next = self.critic(next_states)        # (batch, 1)\n","#         A = r + gamma * V_s_t+1 - V_s_t\n","#         A = rewards + self.gamma * V_next * (1-dones) - V  # TD Error\n","        A = rewards_acc - V                    # MC Error\n","        for _ in range(self.train_actor_epochs): \n","            # actor ===============================================================\n","            log_probs, entropies = self.evaluate_action(states, actions) # same action to calculate right ratios\n","            #(batch, 1) (batch, 1)\n","\n","            # losses.\n","            loss = -A.detach() * log_probs           # (batch, 1) \n","            # larger entropies more exploration\n","            loss_actor = loss - 0.01 * entropies     # (batch, 1)\n","            loss_actor = loss_actor.mean()\n","            self.optimizer_actor.zero_grad()\n","            loss_actor.backward()\n","            self.optimizer_actor.step()\n","\n","            loss_actors.append(loss_actor.item())\n","            loss_entropys.append(entropies.mean().item())\n","\n","            return loss_actors, loss_critics, loss_entropys"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:26:07.958068Z","iopub.execute_input":"2022-05-31T13:26:07.958405Z","iopub.status.idle":"2022-05-31T13:26:07.972053Z","shell.execute_reply.started":"2022-05-31T13:26:07.958369Z","shell.execute_reply":"2022-05-31T13:26:07.971456Z"},"trusted":true,"id":"uJJg5dewJD3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 2000\n","episodes = 5\n","max_ep_len = 300\n","gamma = 0.99"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:26:09.054084Z","iopub.execute_input":"2022-05-31T13:26:09.054934Z","iopub.status.idle":"2022-05-31T13:26:09.05893Z","shell.execute_reply.started":"2022-05-31T13:26:09.054885Z","shell.execute_reply":"2022-05-31T13:26:09.058094Z"},"trusted":true,"id":"lZJJkVwkJD3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actor = Actor(8, 4)\n","critic = Critic(8, 1)\n","optimizer_actor = optim.Adam(actor.parameters(), lr=0.0003)\n","optimizer_critic = optim.Adam(critic.parameters(), lr=0.001)\n","agent_A2C = A2CAgent(actor, critic, optimizer_actor, optimizer_critic, gamma, \n","                     train_actor_epochs=3, \n","                     train_critic_epochs=30)\n","# agent_A2C.load_state_dict(torch.load(\"../input/hw12tmp/agent_A2C.pt\"))"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:26:10.971824Z","iopub.execute_input":"2022-05-31T13:26:10.972461Z","iopub.status.idle":"2022-05-31T13:26:10.980624Z","shell.execute_reply.started":"2022-05-31T13:26:10.972421Z","shell.execute_reply":"2022-05-31T13:26:10.979784Z"},"trusted":true,"id":"Kx8LzygdJD3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","agent_A2C.train()\n","best_avg_total_reward = -np.inf\n","best_test_total_reward = -np.inf\n","avg_total_rewards, avg_final_rewards = [], []\n","total_rewards, final_rewards = deque(maxlen=100), deque(maxlen=100)\n","total_loss_actors, total_loss_critics, total_loss_entropys = deque(maxlen=100), deque(maxlen=100), deque(maxlen=100)\n","progress_bar = tqdm(range(epochs))\n","for epoch in progress_bar:\n","    states_epoch, actions_epoch, log_probs_epoch, rewards_epoch, rewards_acc_epoch, next_states_epoch, dones_epoch = [], [], [], [], [], [], []\n","    for episode in range(episodes):  # Don't infinite loop while learning\n","        states, actions, log_probs, rewards, rewards_acc, next_states, dones = play_one_episode(env, agent_A2C, gamma, max_ep_len)\n","        \n","        states_epoch.extend(states)\n","        actions_epoch.extend(actions)\n","        log_probs_epoch.extend(log_probs)\n","        rewards_epoch.extend(rewards)\n","        rewards_acc_epoch.extend(rewards_acc)\n","        next_states_epoch.extend(next_states)\n","        dones_epoch.extend(dones)\n","        \n","        total_rewards.append(np.sum(rewards))\n","        final_rewards.append(rewards[-1])\n","    \n","    loss_actors, loss_critics, loss_entropys = agent_A2C.learn(states_epoch, \n","                                                               actions_epoch, \n","                                                               next_states_epoch, \n","                                                               rewards_epoch, \n","                                                               rewards_acc_epoch,\n","                                                               dones_epoch)\n","    \n","    avg_total_reward = np.mean(total_rewards)\n","    avg_final_reward = np.mean(final_rewards)\n","    avg_total_rewards.append(avg_total_reward)\n","    avg_final_rewards.append(avg_final_reward)\n","    total_loss_actors.extend(loss_actors)\n","    total_loss_critics.extend(loss_critics)\n","    total_loss_entropys.extend(loss_entropys)\n","    \n","    if best_avg_total_reward < avg_total_reward:\n","#         torch.save(agent_A2C.state_dict(), \"agent_A2C.pt\")\n","        best_avg_total_reward = avg_total_reward\n","    \n","    test_total_reward = test_agent(agent_A2C, env)\n","    if best_test_total_reward < test_total_reward:\n","        torch.save(agent_A2C.state_dict(), \"agent_A2C.pt\")\n","        best_test_total_reward = test_total_reward\n","        print(f\"{epoch:4d}: best test {best_test_total_reward:.4f}, avg:{avg_total_reward:.4f}, best avg:{best_avg_total_reward:.4f}\")\n","    \n","    progress_bar.set_postfix(Total=avg_total_reward, \n","                             Total_best=best_avg_total_reward,\n","                             Test=test_total_reward,\n","                             Test_best=best_test_total_reward,\n","                             Final=avg_final_reward, \n","                             loss_actors=np.mean(total_loss_actors),\n","                             loss_critics=np.mean(total_loss_critics),\n","                             loss_entropys=np.mean(total_loss_entropys))"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:26:31.141531Z","iopub.execute_input":"2022-05-31T13:26:31.141814Z","iopub.status.idle":"2022-05-31T13:59:29.567791Z","shell.execute_reply.started":"2022-05-31T13:26:31.141784Z","shell.execute_reply":"2022-05-31T13:59:29.566814Z"},"trusted":true,"id":"YxZt12IhJD3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","plt.plot(avg_total_rewards)\n","plt.title(\"Total Rewards\")\n","plt.show()\n","\n","plt.plot(avg_final_rewards)\n","plt.title(\"Final Rewards\")\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:59:29.570272Z","iopub.execute_input":"2022-05-31T13:59:29.570615Z","iopub.status.idle":"2022-05-31T13:59:29.982092Z","shell.execute_reply.started":"2022-05-31T13:59:29.570565Z","shell.execute_reply":"2022-05-31T13:59:29.981309Z"},"trusted":true,"id":"uLjNc__dJD3u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing\n","The testing result will be the average reward of 5 testing"],"metadata":{"id":"u2HaGRVEYGQS"}},{"cell_type":"code","source":["# %%script false --no-raise-error\n","\n","# agent = agent_policygradient\n","# agent.load_state_dict(torch.load(\"../input/hw12tmp/agent_policygradient.pt\"))\n","\n","# agent = agent_PPO\n","# agent.load_state_dict(torch.load(\"./agent_PPO.pt\"))\n","\n","# agent = agent_DQN\n","# agent.load_state_dict(torch.load(\"../input/hw12tmp/agent_DQN.pt\"))\n","\n","# agent = agent_A2C\n","# agent.load_state_dict(torch.load(\"./agent_A2C.pt\"))"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:04:24.057736Z","iopub.execute_input":"2022-05-31T01:04:24.058191Z","iopub.status.idle":"2022-05-31T01:04:24.062464Z","shell.execute_reply.started":"2022-05-31T01:04:24.058157Z","shell.execute_reply":"2022-05-31T01:04:24.061528Z"},"trusted":true,"id":"qgMW-kO3JD3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","fix(env, seed)\n","agent.eval()  # set the network into evaluation mode\n","NUM_OF_TEST = 5 # Do not revise this !!!\n","test_total_reward = []\n","action_list = []\n","for i in range(NUM_OF_TEST):\n","    actions = []\n","    state = env.reset()\n","\n","    total_reward = 0\n","\n","    done = False\n","    while not done:\n","        action, _ = agent.action(np.expand_dims(state, axis=0))\n","        actions.append(action.item())\n","        state, reward, done, _ = env.step(action.squeeze())\n","\n","        total_reward += reward\n","\n","    print(total_reward)\n","    test_total_reward.append(total_reward)\n","\n","    action_list.append(actions) # save the result of testing \n","    \n","print(\"mean\", np.mean(test_total_reward))"],"metadata":{"execution":{"iopub.status.busy":"2022-05-31T01:04:38.367605Z","iopub.execute_input":"2022-05-31T01:04:38.368233Z","iopub.status.idle":"2022-05-31T01:04:39.751719Z","shell.execute_reply.started":"2022-05-31T01:04:38.368196Z","shell.execute_reply":"2022-05-31T01:04:39.750583Z"},"trusted":true,"id":"Y0B8TSGgJD3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()\n","\n","fix(env, seed)\n","agent.eval()  # set the network into evaluation mode\n","NUM_OF_TEST = 5 # Do not revise this !!!\n","test_total_reward = []\n","action_list = []\n","for i in range(NUM_OF_TEST):\n","    actions = []\n","    state = env.reset()\n","\n","    img = plt.imshow(env.render(mode='rgb_array'))\n","\n","    total_reward = 0\n","\n","    done = False\n","    while not done:\n","        action, _ = agent.action(np.expand_dims(state, axis=0))\n","        actions.append(action.item())\n","        state, reward, done, _ = env.step(action.squeeze())\n","\n","        total_reward += reward\n","\n","        img.set_data(env.render(mode='rgb_array'))\n","        display.display(plt.gcf())\n","        display.clear_output(wait=True)\n","\n","    print(total_reward)\n","    test_total_reward.append(total_reward)\n","\n","    action_list.append(actions) # save the result of testing \n","\n","virtual_display.stop()"],"metadata":{"id":"5yFuUKKRYH73","execution":{"iopub.status.busy":"2022-05-31T01:05:05.308232Z","iopub.execute_input":"2022-05-31T01:05:05.308555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","print(np.mean(test_total_reward))"],"metadata":{"id":"Aex7mcKr0J01","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Action list"],"metadata":{"id":"leyebGYRpqsF"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","print(\"Action list looks like \", action_list)\n","print(\"Action list's shape looks like \", np.shape(action_list))"],"metadata":{"id":"hGAH4YWDpp4u","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Analysis of actions taken by agent"],"metadata":{"id":"fNkmwucrHMen"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","distribution = {}\n","for actions in action_list:\n","    for action in actions:\n","        if action not in distribution.keys():\n","            distribution[action] = 1\n","        else:\n","            distribution[action] += 1\n","print(distribution)"],"metadata":{"id":"WHdAItjj1nxw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Server \n","The code below simulate the environment on the judge server. Can be used for testing."],"metadata":{"id":"seT4NUmWmAZ1"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","# action_list = np.load(PATH,allow_pickle=True) # The action list you upload\n","seed = 543 # Do not revise this\n","fix(env, seed)\n","\n","agent.eval()  # set network to evaluation mode\n","\n","test_total_reward = []\n","if len(action_list) != 5:\n","    print(\"Wrong format of file !!!\")\n","    exit(0)\n","for actions in action_list:\n","    state = env.reset()\n","    img = plt.imshow(env.render(mode='rgb_array'))\n","\n","    total_reward = 0\n","\n","    done = False\n","\n","    for action in actions:\n","        state, reward, done, _ = env.step(action)\n","        total_reward += reward\n","        if done:\n","            break\n","\n","    print(f\"Your reward is : %.2f\"%total_reward)\n","    test_total_reward.append(total_reward)"],"metadata":{"id":"U69c-YTxaw6b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Your score"],"metadata":{"id":"TjFBWwQP1hVe"}},{"cell_type":"code","source":["%%script false --no-raise-error\n","\n","print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"],"metadata":{"id":"GpJpZz3Wbm0X","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 4pt Baseline 270\n","* 3pt Baseline 170\n","* 2pt Baseline 100\n","* 1pt Baseline 0"],"metadata":{"id":"5kcGNW4pJD33"}},{"cell_type":"markdown","source":["## Reference\n","\n","Below are some useful tips for you to get high score.\n","\n","- [DRL Lecture 1: Policy Gradient (Review)](https://youtu.be/z95ZYgPgXOY)\n","- [ML Lecture 23-3: Reinforcement Learning (including Q-learning) start at 30:00](https://youtu.be/2-JNBzCq77c?t=1800)\n","- [Lecture 7: Policy Gradient, David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)\n"],"metadata":{"id":"wUBtYXG2eaqf"}},{"cell_type":"code","source":[""],"metadata":{"id":"IpTooC0sJD34"},"execution_count":null,"outputs":[]}]}